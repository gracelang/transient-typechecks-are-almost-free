%!TEX root = paper.tex

\section{Evaluation}
\label{sec:evaluation}

\newcommand{\NumIterationsAll}{1000\xspace}
\newcommand{\NumIterationsHiggs}{100\xspace}


To evaluate our approach to gradual type checking,
we first establish the baseline performance of Moth
compared to Java and JavaScript
and then assess the impact of the type checks themselves.

\subsection{Method and Setup}

To account for the complex warmup behavior
of modern systems\citep{Barrett:2017:VMW} as well as
the non-determinism caused by \eg garbage collection and cache effects,
we run each benchmark for \NumIterationsAll iterations in the same
VM invocation.\footnote{
For the Higgs VM, we only use \NumIterationsHiggs iterations,
because of its lower performance.
This is sufficient since Higgs's compilation approach induces less variation
and leads to more stable measurements.}
Afterwards, we inspected the run-time plots over the iterations
and manually determined a cutoff of \WarmupCutOff iterations for warmup,
\ie, we discard iterations with signs of compilation.
As a result, we use a large number of data points to compute the average,
but outliers, caused by \eg garbage collection, remain visible in the plots.
% In this work, we do not consider startup performance,
% because we want to assess the impact of gradual type checks
% on the best possible performance.
All reported averages use the geometric mean since they aggregate ratios.

% Yuria
%  - Ubuntu 16.04.4, Kernel 3.13
%  - 24 hyperthreads
%  - Intel Xeon E5-2620 v3 2.40GHz
% Graal 0.33 Feb. 2018
%
% TODO: update these details when we have time... not a rejection criterion,
% but needs to be correct in the final version
All experiments were executed on a machine running Ubuntu Linux 16.04.4,
with Kernel 3.13.
The machine has two Intel Xeon E5-2620 v3 2.40GHz,
with 6 cores each, for a total of 24 hyperthreads.
We used ReBench 0.10.1\citep{ReBench:2018}, Java 1.8.0\_171, Graal 0.33 (\code{a13b888}),
Node.js 10.4, and Higgs from 9 May 2018 (\code{aa95240}).
Benchmarks were executed one by one to avoid interference between them.
The analysis of the results was done with R 3.4.1,
and plots are generated with ggplot 2.2.1 and tikzDevice 0.11.
Our experimental setup is available online to enable reproductions.\footnote{\url{https://github.com/gracelang/moth-benchmarks}}


\subsection{\AWFY?}
\label{sec:baseline-perf}


\begin{figure}
  \centering
	\AwfyBaseline{}
	\caption{Comparison of Java 1.8, Node.js 10.4, Higgs VM, and Moth.
  The boxplot depicts the peak-performance results for the \AWFY benchmarks,
  each benchmark normalized individually based on the result for Java,
  which means all results for Java are 1.0, and its box appears as a line.
  The dots on the plot represent the geometric mean reported as averages.
  For these benchmarks, Moth is within the performance range
  of JavaScript, as implemented by Node.js,
  which makes Moth an acceptable platform for our experiments.
  }
	\label{fig:awfy-baseline}
\end{figure}

% - setting a base line with the AWFY benchmarks
% - comparing
%   - Java
%   - Node.js
%   - Moth
%   - Higgs
To establish the performance of Moth,
we compare it to Java and JavaScript.
Moth is used in its untyped version, \ie, without type checks.
For JavaScript we chose two implementations,
Node.js with V8 as well as the Higgs VM.
The Higgs VM is an interesting point of comparison,
because Richards\,\emph{et\,al.}\citep{Richards2017} used it in their study.
The goal of this comparison is to determine whether our approach
could be applicable to industrial strength language implementations
without adverse effects on their performance.

We compare across languages based on the \AWFY benchmarks\citep{Marr2016},
which are designed to enable a comparison
of the effectiveness of compilers across different languages.
To this end, they use only a common set of core language elements.
While this reduces the performance-relevant differences between languages,
the set of core language elements covers only common object-oriented language
features with first-class functions.
Consequently, these benchmarks are not necessarily a predictor
for application performance,
but can give a good indication for basic mechanisms such as type checking.
%% SM: removed the educational bit, from the emails I gather that this is not a
%%     desired story line any longer
% Furthermore, in an educational setting,
% we assume that students will focus on using these basic language features
% as they learn a new language.
% - arguing that Moth has state of the art performance on the given benchmarks
% - this is a reasonable foundation to make performance claims
%   that can generalize to state-of-the-art custom VMs such as V8

\Cref{fig:awfy-baseline} shows the results.
We use Java as baseline since it is the fastest language implementation
in this experiment.
Note that we perform a unit conversion on the results separately for each benchmark,
using the average of Java as 1 unit.
While this conversion does not change the distribution of the data,
it allows us to show it neatly on one plot.

We see that Node.js (V8) is about
\OverheadNodeGMeanX (min. \OverheadNodeMinX, max. \OverheadNodeMaxX)
slower than Java.
Moth is about \OverheadMothGMeanX (min. \OverheadMothMinX, max. \OverheadMothMaxX) slower than Java.
As such, it is on average \OverheadMothNodeGMeanP (min. \OverheadMothNodeMinP, max. \OverheadMothNodeMaxX) slower than Node.js.
Compared to the Higgs VM, which is on these benchmarks
\OverheadHiggsGMeanX (min. \OverheadHiggsMinX, max. \OverheadHiggsMaxX) slower than Java,
Moth reaches the performance of Node.js more closely.
With these results, we argue that Moth is a suitable platform to
assess the impact of our approach to gradual type checking,
because its performance is close enough to state-of-the-art VMs,
and run-time overhead is not hidden by slow baseline performance.


\subsection{Performance of Transient Gradual Type Checks}

% - benchmark selection
%  - dynamic type checking performance determined based on commonly used
%    benchmarks from the gradual typing papers
% - we are subsetting, need to explain why
%   - the subset of benchmarks that we managed to port to Grace/Moth

The performance overhead of our transient gradual type checking system
is assessed based on the \AWFY benchmarks
as well as benchmarks from the gradual-typing literature.
The goal was to complement our benchmarks with additional ones that are
used for similar experiments and can be ported to Grace.
To this end, we surveyed a number of papers\citep{Takikawa2016,Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Stulova2016,Greenman2018}
and selected benchmarks that have been used by multiple papers.
Some of these benchmarks overlapped with the \AWFY suite,
or were available in different versions.
While not always behaviorally equivalent,
we chose the \AWFY versions since we already used them to
establish the performance baseline.
The selected benchmarks as well as the papers in which they were used are shown in
\cref{tab:gradual-benchmarks}.

\begin{table}[htb]
  \caption{Benchmarks selected from literature.}
  \label{tab:gradual-benchmarks}
  \begin{center}
    \begin{tabular}{l l r}
      Fannkuch & \cite{Vitousek2017,Greenman2018} \\
      Float & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
      Go & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
      NBody & \cite{Kuhlenschmidt:2018:preprint,Vitousek2017,Greenman2018} & used \cite{Marr2016} \\
      Queens & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} & used \cite{Marr2016} \\
      PyStone & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
      Sieve & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017,Greenman2019jfp} & used \cite{Marr2016} \\
      Snake & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017,Greenman2019jfp} \\
      SpectralNorm & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
    \end{tabular}
  \end{center}
\end{table}

The benchmarks were modified to have complete type information.
To ensure correctness and completeness of these experiments,
we added an additional check to Moth that
reports absent type information to ensure each benchmark is fully typed.
To assess the performance overhead of type checking,
we compare the execution of Moth with all checks disabled, \ie, the baseline version from
\cref{sec:baseline-perf}, against an execution that has all checks enabled.
We did not measure programs that mix typed and untyped code
because with our implementation technique a fully typed program is expected to
have the largest overhead.

\paragraph*{Peak Performance}

\Cref{fig:typing-overhead} depicts
the overall results comparing Moth,
with all optimizations,
against the untyped version.
The run-time overhead,
after discarding the warmup iterations,
is on average
\OverheadTypingGMeanP (min. \OverheadTypingMinP, max. \OverheadTypingMaxP).

\begin{figure}[htb]
  \centering
	\TypingOverhead{}
	\caption{A boxplot comparing the performance of Moth
  with and without type checking.
  The plot depicts the run-time overhead on peak performance over
  the untyped performance. On average, transient type checking introduces
  an overhead of \OverheadTypingGMeanP (min. \OverheadTypingMinP, max. \OverheadTypingMaxP).
  The average is indicated as a line with long dashes.
  The visible outliers correspond to various complex aspects of the overall
  system, \eg, including garbage collection and cache effects.
  Note that the axis is logarithmic to avoid distorting the proportions
  of relative speedups and slowdowns.
  }
	\label{fig:typing-overhead}
\end{figure}

% explain maxima

The benchmark with the highest overhead of \OverheadListP is List.
The benchmark traverses a linked list and
has to check the list elements individually.
Unfortunately, the structure of this list introduces checks
that do not coincide with shape checks on the relevant objects.
We consider this benchmark a pathological case and discuss it
in detail in \cref{sec:disc-pathological-case}.

Beside List, the highest overheads are on
Richards (\OverheadRichardsP), CD (\OverheadCDP),
Snake (\OverheadSnakeP), and Towers (\OverheadTowersP).
Richards has one major component, also a linked list traversal,
similar to List.
Snake and Towers primarily access arrays in a way that introduces checks
that do not coincide with behavior in the unchecked version.

% explain minima

In some benchmarks, however, the run time decreased; notably Permute (\OverheadPermuteP),
GraphSearch (\OverheadGraphSearchP), and Storage (\OverheadStorageP).
Permute simply creates the permutations of an array.
GraphSearch implements a page rank algorithm
and thus is primarily graph traversal.
Storage stresses the garbage collector by constructing a tree of arrays.
For these benchmarks the introduced checks seem to coincide with shape-check operations
already performed in the untyped version.
The performance improvement is possibly caused by having checks earlier,
which enables the compiler to more aggressively move them out of loops.
Another reason could simply be that the extra checks shift the boundaries
of compilation units.
In such cases, checks might not be eliminated completely,
but the shifted boundary between compilation units might mean that
the generated native code interacts better with
the instruction cache of the processor.
% Such shifts in performance of about 10\%
% are somewhat common for highly optimizing just-in-time compilers.\mwh{cite?}
% \kjx{unfortuantely all the best citations give more detailed
  % benchmarking methodologies (Georges, Kalibera) that we don't follow.}

\paragraph*{Warmup Performance}

To more precisely measure warmup, all relevant experiments were executed 30 times,
with each running for 100 iterations.
The resulting \cref{fig:typing-warmup} shows the first 100 iterations for each benchmark.
For each iteration $n$, we normalized the measurements to the mean of iteration $n$
of the untyped Moth implementation.
Thus, any increase indicates a slow down because of typing.
The darker lines indicate the means, while the lighter area indicates
a $95\%$ confidence interval.

\begin{figure}[htb]
  \centering
  %\TypingWarmup{}
  \TypingWarmupConfInterval{}
	\caption{Plot of the run time for the first 100 iterations.
           The lines indicate the mean at iteration $n$ normalized
           to the untyped result, the lighter area indicates a $95\%$ confidence interval.
           The first iteration, \ie, mostly interpreted, seems
           to be affected significantly only for Mandelbrot, though
           CD shows slower behavior in early warmup, too.}
	\label{fig:typing-warmup}
\end{figure}

Looking only at the first few iterations,
where we assume that most code is executed in the interpreter and might be
affected by compilation activity,
the overhead appears minimal.
Only the Mandelbrot and CD benchmarks shows a noticeable slowdown.

Mandelbrot with its distinctly slow first iteration
can be explained by its code structure.
Since it is a computational kernel with many primitive operations,
but no method calls, optimized code is only reached after the first full
benchmark iteration.
The problem could be alleviated with on-stack-replacement
for loops, which is currently not done.
Since other benchmarks use methods, they reach compiled code earlier
and do not exhibit the same first-iteration slowdown.

PyStone however show various spikes.
Since spikes appear in both directions (speedups and slowdowns),
we assume that they indicate a shift,
for instance, of garbage collection pauses,
which may happen because of different heap configurations
triggered by the additional data structures for type information.

\subsection{Effectiveness of Optimizations}

To characterize the concrete impact of our two optimizations --- \ie,
the optimized type checking node that replaces complex type tests
with checks for object shapes and our matrix to cache sub-typing information,
--- we look at the number of type checks performed by the benchmarks
as well as the impact on peak performance.

\paragraph*{Impact on Performed Type Tests}

\Cref{tab:type-statistics} gives an overview of the number of type tests done
by the benchmarks during execution.
We distinguish two operations \code{check\_generic} and \code{is\_subtype\_of},
which correspond to the operations in
 \cref{ex:typenode:generic} and \cref{ex:type:satisfied} of \cref{ex:type}.
Thus, \code{check\_generic} is the test called
whenever a full type check has to be performed,
and \code{is\_subtype\_of} is the part of the check that
determines the relationship between two types.
The second column of \cref{tab:type-statistics} indicates
which optimization is applied,
and the following columns show the mean,
minimum, and maximum number of invocations of the tests
over all benchmarks.

\begin{table}[htb]
  \caption{Type Test Statistics over all Benchmarks.
  This table shows how many of the type tests can be avoided based on our two optimizations.
  As indicated by the numbers, the number of type tests can vary significantly
  between benchmarks.
  Thus, the table shows the mean, minimum, and maximum number of type tests
  across all benchmarks for a given configuration.
  With the use of an optimized node that replaces type checks with simple object shape checks,
  \code{check\_generic} is invoked only for the first time that a lexical location
  sees a specific object shape, which eliminates run-time type checks almost completely.
  Using our subtype matrix that caches type-check results,
  invocations of \code{is\_subtype\_of} are further reduced by an order of magnitude.}
  \label{tab:type-statistics}

  \begin{center}
    \TypingStatsTable{}
  \end{center}
\end{table}

The baselines without optimizations are the rows with the results
for neither of the optimizations being enabled.
Depending on the benchmark,
we see that the type tests
are done tens of millions
to hundreds of millions times
for a single iteration of a benchmark.

Our optimizations reduce the number of type test invocations dramatically.
As a result, the full check for the subtyping relationship is done only once for
a specific type and super type.
Similarly, the generic type check is replaced by a shape check
and thus reduces the number of expensive type checks
to the number of lexical locations that verify types
combined with the number of shapes a specific lexical
location sees at run time.

\paragraph*{Impact on Performance}
\label{sec:eval-opt-perf}

\Cref{fig:perf-impact-optimization} shows how our optimizations contribute
to the peak performance.
The figure depicts Moth's peak performance over
all benchmarks, depending on the activated optimizations.
As for \cref{fig:awfy-baseline},
we do a per-benchmark unit conversion using Moth (untyped),
preserving the distribution properties of the results,
but enabling us to show the results on a single plot.

As seen before in \cref{fig:typing-overhead}, the untyped version is faster by \OverheadTypingGMeanP.
Moth with both optimizations enabled as well as
Moth with the optimized type-check node (cf. \cref{ex:type})
reach the same performance.
This indicates that the subtype cache matrix is not strictly necessary for
the peak performance.
However, we can see that the subtype cache matrix improves performance
by an order of magnitude over the Moth version without any type check optimizations.
This shows that it is a relevant and useful optimization.
Based on the numbers of \cref{tab:type-statistics},
we see that this optimization is relevant for the very first execution of code.
For code that has not executed before,
having the global cache for the subtype relations gives the most benefit.
After the first execution, the lexical caches in form of the type check nodes
are primed with the same information, and the subtype cache matrix is only rarely needed.
An example for code that benefits from the subtype cache matrix is unit test code,
because most of the code is executed only once.
While the performance of unit tests is not always critical,
it can have a major impact on developer productivity.

\begin{figure}[htb]
  \centering
	\OptimizationOverview{}
  \caption{Performance Impact of the Optimizations on the Peak Performance over all benchmarks.
  The boxplot shows the performance of Moth normalized to the untyped version, \ie,
  without any type checks.
  This means all results for Moth (untyped) are 1.0 and its box appears as a line.
  The dots on the plot represent the geometric mean reported as averages.
  The performance of Moth with both optimizations and Moth
  with only the node for optimized type checks are identical.
  The subtype check cache improves performance over the unoptimized version,
  but does not contribute to the peak performance.
  }
	\label{fig:perf-impact-optimization}
\end{figure}

\paragraph*{Impact on Memory Usage}

In our implementation, the subtype cache matrix is the largest additional
data structure.
We initialize it for up to 1000 types and use 1 byte per type combination.
Java utilizes ca.\ 1MB of memory for the matrix.
Additional memory is used to represent the type-check nodes at every lexical location.
Since they behave like polymorphic inline caches (PIC)\citep{Hoelzle:91:PIC},
their memory usage depends on the specific program execution.
For the benchmarks used in this paper, the extra memory use can be up to 200KB.

In the context of Graal and Truffle, this additional memory usage is small,
since the metacompilation approach uses a lot of memory\citep{Marr:2015:MTPE}.
In a dedicated virtual machine,
memory use can be further optimized and be as efficient as for other kinds of PICs.

% Havlak     116424 byte nodes
% Mandelbrot  24480 byte
% DeltaBlue  134784 byte
% Richards    67824 byte
% Json        59760 byte
% CD         174168 byte

\subsection{Transient Typechecks are (Almost) Free}
\label{sec:eval-best-case}

As discussed in the introduction, in many existing gradually typed systems,
one would expect a linear increase of the performance overhead
with respect to an increasing number of type annotations.

In this section, we show that this is not necessarily the case on our system.
For this purpose we use two microbenchmarks, Check and Nest,
which have at their core method calls with 5 parameters.
The Check benchmark calls the same method 10 times in a row, \ie, it has 10 call sites.
The Nest benchmark has 10 methods with identical signatures,
which recurse from the first one to the last one.
Thus, there are still 10 method calls, but they are nested in each other.
In both benchmarks, each method increments a counter,
which is checked at the end of the execution to verify that both do the same
number of method activations, and only the shape of the activation stack differs.

Each benchmark exists in six variants, each variant in a separate file,
going from having no type annotations
over annotating only the first method parameter to annotating all 5 parameters.
To demonstrate the impact of compilation,
we present the results for the first iteration as well as the hundredth iteration.
The first iteration is executed at least partially in the interpreter,
while the hundredth iteration executes fully compiled.

\Cref{fig:type-cost-micro} shows that such a common scenario of methods being
gradually annotated with types
does not incur an overhead on peak performance in our system.
The plot shows the mean of the run time for each benchmark configuration.
Furthermore, it indicates a band with the 95\% confidence interval.
The yellow line, Moth (neither), corresponds to our Moth with type checking
but without any optimizations.
For this case, we see that the performance overhead grows
linearly with the number of type annotations.

For Moth (both) and Moth (untyped), we see for the first iteration that
the band of confidence intervals diverges, indicating that the additional type
checks have an impact on startup performance.
In contrast the confidence intervals overlap for the hundredth iteration,
which shows that Moth does not suffer from
a general linear overhead when adding type checks.
Instead, most type checks do not have an impact on peak performance.
However, as previously argued for the List benchmark,
this is only the case for checks that can be subsumed by shape checks
(shape checks are performed whether or not type checks are present).

\begin{figure}
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \TypeCostFirstIt{}
    \caption{Iteration 1.}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \TypeCostLastIt{}
    \caption{Iteration 100.}
  \end{subfigure}

	% \TypeCost{}
  \caption{Transient Typechecks are (Almost) Free.
    Two microbenchmarks, each with six variants, demonstrate the common scenario
    of adding type annotations over time, which in our system does not have an impact on peak performance.
    The benchmark variants differ only in the increasing number
    of method arguments that have type annotations.
    We show the result for the first benchmark iteration (a) and the
    one hundredth (b).
    Moth (neither), \ie, Moth without our two optimizations sees a linear increase in run time.
    For the first iteration, we see some difference between Moth (both) and Moth (untyped).
    By the hundredth iteration, however, the compiler has eliminated
    the overhead of the type checks
    and both Moth variants essentially have the same performance
    (independent of the number of method arguments with type annotations).}
	\label{fig:type-cost-micro}
\end{figure}

\subsection{Changes to Moth}
\label{sec:eval-changes}

Outlined earlier in \cref{sec:method}, a secondary
goal of our design was to enable the implementation of our approach to be
realized with few changes to the underlying interpreter.
This helps to ensure that each Grace implementation
can provide type checking in a uniform way.

By examining the history of changes maintained by our version control,
we estimate that our implementation of Moth required
549 new lines and 59 changes to existing lines.
The changes correspond to the implementation of
new modules for the type class (179 lines) and
the self-specializing type checking node (139 lines),
modifications to the front end to extract typing information
(115 new lines, 14 lines changes)
and finally the new fields and amended constructors for AST nodes
(116 new lines, 45 lines changes).

% New Classes
% -----------
% TypeCheckNode               (139 new lines)
% SomStructuralType           (179 new lines) = 318 new

% Parsing
% -------
% returnType                  ( 18 new lines)
% typeFor                     ( 31 new lines)
% typesForParameters          ( 27 new lines)
% typesForLocals              ( 11 new lines)
% parseTypeLiteral            ( 19 new lines)
% invoke parseTypeLiteral     (  6 new lines)
% adding return type          (  3 new lines)
% types for declarations      (  4 changed lines)
% adding type to parameters   (  4 changed lines)
% adding type locals          (  4 changed lines)
% adding type to variable     (  2 changed lines) = 115 new, 14 changed

% AST
% ---
% Storage location            ( 20 changed lines)
% local variable read/write   ( 28 new lines,  1 changed)
% add type to slot read/write (  4 new lines,  2 changed)
% cached TX slot read/write   ( 14 new lines, 21 changed)
% dispatch                    ( 31 new lines,  1 changed)
% class factory getTyoe       ( 39 new lines)     = 116 new, 45 changed

