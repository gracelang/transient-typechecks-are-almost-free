%!TEX root = paper.tex

\section{Gradual Types in Grace}
\label{sec:background}

This section introduces Grace, and 
motivates supporting transient gradual typing in the language.

\subsection{The Grace Programming Language}
\label{ssec:grace}

Grace is an object-oriented, imperative, educational programming language,
with a focus on introductory programming courses,
but is also intended for more advanced study and research\citep{graceOnward12,graceSigcse13}.
%
While Grace's syntax draws
from the so-called ``curly bracket'' tradition of C, Java, and
JavaScript, the structure of the language
is in many ways closer to Smalltalk:
all computation is performed via dynamically dispatched
``method requests'' where the object receiving the request
decides which code to run;
returns within lambdas are ``non-local'', returning to the method
activation in which the block is instantiated\citep{bluebook}. 
In other ways, Grace is closer to JavaScript than Smalltalk: Grace
objects can be created from object literals, rather than by
instantiating classes\citep{Black2007-emeraldHOPL,JonesECOOP2016}
and objects and classes can be deeply nested within each 
other\citep{betabook}.

Critically, Grace's declarations and methods' arguments
and results can be annotated with types, and those types can be checked
either statically or dynamically. This means the type system is
intrinsically gradual:%
%
~type annotations should not affect the semantics of a correct
program\citep{XXXSiek2015}, and the type system
includes a distinguished ``\code{Unknown}'' type which matches any other type
and is the implicit type for untyped program parts.

The static core of Grace's type system is well described
elsewhere\citep{TimJonesThesis};
here we explain how
these types can be understood 
dynamically, from the Grace programmer's point of view.
Grace's types are structural \citep{graceOnward12},
that is, an object implements a type whenever it
implements all the methods required by that type,
rather than requiring classes or objects to declare types explicitly.
Methods match when they have the same name and arity:
argument and return types are ignored.
A type thus expresses the requests an object can respond to,
for example whether a particular accessor is available,
rather than a nominal location in an explicit inheritance hierarchy.

Grace then checks the types of values at run time:
%
\begin{itemize}
\item the values of arguments are checked after a method is requested, 
      but before the body of the method is executed;
\item the value returned by a method is checked after its body is executed; and
\item the values of variables are checked
      whenever written or read by user code.\footnote{
        Our rational for checking reads in addition to writes
        is described in \cref{sec:discussion-read-write}.}
\end{itemize}
%
%
%% Our gradual type checks for Grace are designed to be simple,
%% straightforward, and easy to understand:
%% \begin{itemize}
%%   \item types are sets of methods (first-order structural typew)
%%   \item all gradual type annotations are checked at run time
%%   \item failing run-time type checks terminate execution
%% \end{itemize}
%
%
In the spectrum of gradual typing, these semantics are
closest to the
transient typechecks of Reticulated Python
\cite{reticPython2014,Greenman2018}.
Reticulated Python inserts transient checks only when a value flows from untyped
to typed code, while Grace inserts transient checks only at explicit type
annotations (but in principle checks every annotation every time).



\subsection{Why Gradual Typing?}


Our primary motivation for this work
is to provide a flexible system 
to check consistency between an execution of a program
and its type annotations.
%% KJX removed cos performance is in the second list below
%% without significantly impacting run-time performance.
A key part of the design philosophy of Grace is that the language should not force
students to annotate programs with types until they are ready, so that
teachers can choose whether to introduce types early, late, or even
not at all. 

A secondary goal is to have a design that can be implemented with
only a small set of changes to facilitate integration in existing systems.

Both of these goals are shared with
much of the other work on gradual type systems,
but our context leads to some different choices.
First,
while checking Grace's type annotations statically may be optional,
checking them dynamically should not be:%
~any value that flows into a variable, argument, or result
annotated with a type must conform to that type annotation.
%% This means our focus is not to devise a sound typing approach,
%% but rather an approach that ensures that the observed execution matches
%% the one the Grace programmer expects when considering a program's type annotations.
Second, 
adding type annotations should not degrade a program's performance,
or rather, programmers should not be encouraged to
improve performance by removing type annotations.
And third, 
we allow the programmer to execute a program even when not statically type-correct.
Allowing such execution is useful to students,
where they can see concrete examples of dynamic type errors.
This is possible because Grace's static type checking is optional,
which means that an
implementation cannot depend on the correctness or mutual
compatibility of a program's type
annotations.


Existing gradual type
implementations do not meet these goals, particularly regarding
performance; hence the ongoing debate about whether gradual typing is
alive, dead, or some state
in between\citep{Takikawa2016,Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Greenman2018,Greenman2019jfp}.


\subsection{Using Grace's Gradual Types}

%\rr{We should consider a more complex example for the camera ready (so that it's not stretched too thin for the comment on avoiding checking the argument types.)}
We now illustrate how the gradual type checks work in practice
in the context of a simple program to record information about vehicles.
Suppose the programmer starts developing this vehicle
application by defining an object intended to represent a car
(\cref{lst:car-reg}, \cref{ex:object}) and writes a method that, given
the car object, prints out its registration number (\cref{ex:method}).
%

\begin{lstlisting}[caption={The start of a simple Grace program for tracking vehicle information.},%
  float=h,label=lst:car-reg,escapechar=|,columns=flexible]
def car = object {|\label{ex:object}|
    var registration is public := "JO3553"
}

method printRegistration(v) {|\label{ex:method}|
    print "Registration: {v.registration}"
}
\end{lstlisting}



\begin{lstlisting}[label={ex:vehicle},caption={Adding a type annotation to a method parameter.},escapechar=|,columns=flexible,float,floatplacement=H]
type Vehicle = interface { |\label{ex:adding-type:vehicle}|
    registration    
}

method printRegistration(v: Vehicle) { |\label{ex:adding-type}|
    print "Registration: {v.registration}"
}
\end{lstlisting}

Next, the programmer adds a check to ensure any object passed to the
\code{print\-Reg\-is\-tra\-tion} method will respond to the
\code{registration} request; 
they define the structural type \code{Vehicle}\citep{theCleanVehicle}
naming just that method (\cref{ex:vehicle}, \cref{ex:adding-type:vehicle}), 
and annotate the \code{print\-Reg\-is\-tra\-tion} method's
argument with that type (\cref{ex:vehicle}, \cref{ex:adding-type}).
The annotation ensures that a type error will be thrown if an object,
passed to the \code{print\-Reg\-is\-tra\-tion} method,
cannot respond to the \code{registration} message.
Without the type check, the \code{print} method would
cause a run-time error when interpolating the string.
However, since type errors cause termination, 
the run-time error in the middle of the
\code{print} implementation
will now be avoided.

In \cref{ex:complex}, 
the programmer continues development and creates two car objects 
(\cref{ex:personal-car,ex:government-car}),
that conform to an expanded \code{Vehicle} type (\cref{ex:new-vehicle}).


\begin{lstlisting}[caption={A program in development with inconsistently
    typed \code{registerTo} methods.},escapechar=|,label={ex:complex},float,floatplacement=htb,columns=flexible,float,floatplacement=H]
type Vehicle = interface { |\label{ex:new-vehicle}|
    registration
    registerTo(_)
}

type Person = interface { name }
type Department = interface { code }

var personalCar : Vehicle := |\label{ex:personal-car}|
  object {
    var registration is public := "DLS018"
    method registerTo(p: Person) {|\label{ex:personal-car:registerTo}|
      ...
      print "{self} is now registered to {p.name}"
    } 
  }

var governmentCar : Vehicle := |\label{ex:government-car}|
  object {
    var registration is public := "FKD218"
    method registerTo(d: Department) { |\label{ex:government-car:registerTo}|
      print "{self} is now registered to {d.code}"
    }
  }

governmentCar.registerTo( |\label{ex:invoke-register-to}|
  object {
    var name is public := "Richard"
  }
)
\end{lstlisting}


Note that each version of the \code{registerTo} method
declares a different type for its parameter
(\cref{ex:personal-car:registerTo,ex:government-car:registerTo}).
%
When the programmer executes this program,
both \code{personal\-Car} and \code{governmentCar} pass the 
type check for \code{Vehicle}. Both objects respond to
\code{registeration} and \code{registerTo}. Notably,
the type of the argument for \code{registerTo} is ignored.
%
At \cref{ex:invoke-register-to} the developer
attempts to register the government car to an object representing a person:%
~only when the method (\cref{ex:government-car:registerTo}) is \emph{invoked}
will the gradual type test on the argument fail
(the object passed in is not a \code{Department} because it lacks a
\code{code} method).

\section{Graal, Truffle, Self-Optimization and Dynamic Adaptive Compilation}
\label{sec:background-implementation}

This section gives a brief introduction into just-in-time compilation,
and the main techniques we rely on for our optimizations.

\subsection{Self-Optimizing Interpreters}
\label{sec:background-self-opt}

Self-optimizing abstract-syntax-tree (AST) interpreters\citep{Wurthinger:2012:SelfOptAST}
are the foundation for the work presented here.
Together with partial evaluation\citep{Wurthinger:2017:PPE},
self-optimization enables efficient dynamic language implementations
that reach the performance of custom state-of-the-art virtual machines
(cf. \cref{sec:baseline-perf} and \citep{Marr:2015:MTPE}).
The framework for building such interpreters is called Truffle.

The key idea is that an AST rewrites itself based on a program's run-time values
to reflect the minimal set of operations needed to execute the program correctly.

As an example, consider the addition of two numbers in a dynamic language,
possibly written simply as: \code{a + b}.
Because there are no static types known,
the run-time values for \code{a} and \code{b} could potentially be anything
from an integer or a double, to a string or a collection, or any
arbitrary objects that have a ``\code{+}'' method.
In an self-optimizing interpreter, the expression may be represented by
an \code{add} node, with two child nodes that each read a variable.
The first time the \code{add} node executes,
it may find that both values to be added are integers.
It will then speculate that all future executions
also see integers, and thus, rewrite itself to an \code{add-integer} node.
This \code{add-integer} node will simply confirm that both values are integers,
and then directly perform the integer addition.
Compared to a general \code{add} node,
we do not have to cover the cases for doubles, strings, and other kinds of objects,
which results in much simpler code that can be more easily optimized.
All other cases are supported by rewriting the \code{add} node to more general
versions.
This happens, for instance, when the values are not integers. 
However, in practice, programs tend be monomorphic and
so such speculation is highly beneficial.

What starts out as something close to a traditional AST,
in the end, incorporates additional knowledge about execution.
As a consequence of this rewriting, such trees should be referred
to more correctly as \emph{execution trees} rather than ASTs.


\subsection{Polymorphic Inline Caches for Optimizing Dynamic Behavior}
\label{sec:background-pics}

Polymorphic inline caches (PICs)\citep{Hoelzle:91:PIC}
are a variation on the theme of caching run-time values to improve performance.
Originally, they focused on method invocation in dynamic languages
to avoid costly method lookups by
caching the looked-up method for a specific type.
%
For dynamic languages,
PICs can be generalized to not only consider the receiver type,
but instead the object shape (cf. \cref{sec:background-obj-shape}),
which enables the optimizations we are aiming for.

In a language such as JavaScript,
a PIC could be used for the following expression:
\code{obj.toString()}.
The dot can be thought of as the lexical representation of the method lookup.
An implementation would keep a small cache for each such dot in the code.
This means, for each lexical lookup location, we have a separate cache.
PICs benefit from the relatively monomorphic behavior of programs.
A specific lexical lookup is likely to see only one kind of
object in the \code{obj} variable, so the cache will usually have the
correct method for the object ready 
and can avoid a costly lookup.


\subsection{Object Shapes: Metadata for Dynamic Objects}
\label{sec:background-obj-shape}

Object shapes\citep{woss2014object},
which are also know as maps\citep{Self} or hidden classes,
are in the most general case a type of usage profile for groups of objects.
In languages such as Self, JavaScript, and Grace,
we do not have traditional classes that define the set of fields for an object.
The set of fields might even change over time.
Furthermore, fields can theoretically store any possible value.
However, in practice, the behavior of programs is again relatively monomorphic
and objects created in a specific part of a  program are likely
to have always the same set of fields, which each are used to store only
a small number of different kinds of values.
For example, an object representing a counter would have a field \code{count},
which always stores integers,
while an object representing a person may have always a field \code{name}
that stores a string, but never an integer.

Object shapes represent this run-time information in a way that allows a
just-in-time compiler to
represent objects in memory similarly to C structs, and then to 
generate highly efficient code.
Object shapes can be conflated with additional information,
for instance to represent knowledge about types\citep{Bauman2017,Richards2017}.
Object shapes are useful for indexing PICs,
since they give objects a form of identification that groups them.
Similar to classes, shapes tend to monomorphic in practice.


\subsection{Just-in-Time Compilation with Graal and Truffle}
\label{sec:background-graal-pe}

The Graal compiler is a just-in-time compiler for Java.
For languages built on the Truffle framework,
Graal comes with  additional support for partial evaluation,
which enables efficient native code generation for
Truffle interpreters\citep{Wurthinger:2017:PPE}. As such, Graal is a metacompiler.
This means that instead of compiling a specific program, in our case a Grace program,
Graal compiles our Grace interpreter Moth for the execution of a specific Grace method.

For simplicity, partial evaluation can be thought of a highly aggressive inlining strategy.
It starts with the root node of a specific Grace method
and inlines all code reachable from it, while considering the execution tree
to be constant.
To enable further optimizations ---% 
constant folding, common subexpression elimination, loop-invariant code motion, and more ---%
Graal does further inlining on the level of the Grace program,
which is important to expose the same optimization opportunities that
classic just-in-time compilers use.

% Especially loop-invariant code motion and common subexpression elimination are important
% to generate efficient native code for dynamic languages.
% Since we rely on techniques such as self-optimizing nodes, PICs and object shapes,
% which all introduce various checks,
% a compiler needs to move these out of loops, and remove redundant checks.

By combining all the techniques sketched in this section,
Graal and Truffle are able to execute dynamic languages as efficiently
as virtual machines built for a specific language -- but with much
less implementation effort.
