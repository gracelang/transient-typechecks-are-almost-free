%!TEX root = paper.tex

\section{Moth: Grace on Graal and Truffle}
\label{ssec:moth}
\label{sec:method}

Implementing dynamic languages as state-of-the-art virtual machines
can require enormous engineering efforts.
Meta-compilation approaches\citep{Marr:2015:MTPE}
such as RPython\citep{Bolz:2009:TMP,Bolz:2013:IMT}
and GraalVM\citep{Wurthinger2013,Wurthinger:2017:PPE}
reduce the necessary work dramatically,
because they allow language implementers to leverage existing VMs
and their support for just-in-time compilation and garbage collection.

Moth\citep{Roberts2017} adapts \SOMns\citep{SOMns} to leverage this infrastructure for Grace.
%\citeurl{\SOMns.}{SOMns: A Newspeak for Concurrency Research}{Stefan Marr}{}{https://github.com/smarr/SOMns/}
\SOMns is a Newspeak implementation\citep{Bracha:10:NS} on top of the Truffle framework and the Graal just-in-time compiler,
which are part of the GraalVM project.
One key optimization of \SOMns for this work is the use of
object shapes\citep{woss2014object},
also known as maps\citep{Self} or hidden classes.
They represent the structure of an object and the types of its fields.
In \SOMns, shapes correspond to the class of an object and augment it with
run-time type information.
%
%
With Moth's implementation,
\SOMns was changed to parse Grace code,
adapting a few of the self-optimizing abstract-syntax-tree nodes
to conform to Grace's semantics.
Despite these changes Moth
preserves the peak performance of \SOMns,
which reaches that of V8,
Google's JavaScript implementation
(cf. \cref{sec:baseline-perf} and Marr~\textit{et~al.}~\cite{Marr2016}).

\subsection{Implementation} 
\label{ssec:implementation} 

%This section gives an overview of a possible implementation
%based on an abstract-syntax-tree (AST) interpreter.

We have implemented our gradual type checks 
by extending Moth, 
an abstract-syntax-tree (AST) interpreter for
Grace (\cref{ssec:moth}).
%
% We developed our implementation as an extension to Moth.
% As described earlier in \cref{ssec:moth},
% Moth is an AST-based interpreter on top of the Graal VM.
%
% It is optimizes itself based on for instance run-time type information.
%
%
%Based on \cref{sec:term-type-error},

% \kjxdone{why both reading and writing?  answer - doesn't matter?}

One of the goals for our approach to gradual typing was to keep
the necessary changes to an existing implementation small,
while enabling optimization in highly efficient language runtimes.
%
In an AST interpreter, we can implement this approach by attaching the
checks to the relevant AST nodes: the expected types for the argument
and return values can be included with the node for requesting a
method, and the expected type for a variable can be attached to the
nodes for reading from and writing to that variable.  In practice, we
encapsulate the logic of the check within a new class of AST
nodes, specially to support gradual type checking.  Moth's front end was adapted to parse and record type
annotations and attach instances of this checking node as children of the
existing method, variable read, and variable write nodes.


% The check node is detailed in \cref{ssec:optimization} to discuss
% relevant optimizations.

%

The check node uses the internal representation of a Grace type
(cf. \cref{ex:type}, \cref{ex:type:check}) to test whether an observed
object conforms to that type. 
% These \code{Type} objects are created by
% Grace \code{interface} expressions, and also help
% to support Grace's pattern matching facilities \cite{gracePatternsDLS12}.
An object satisfies a type if all members required by the type are provided
by that object (\cref{ex:type:satisfied}).


\begin{lstlisting}[label={ex:type},escapechar=|,caption={Sketch of a \code{Type} in our system and its \code{check()} semantics.},float,floatplacement=htb,columns=flexible,float,floatplacement=H]
class Type:
  def init(members):
    self._members = members

  def is_satisfied_by(other: Type): |\label{ex:type:satisfied}|
    for m in self._members:
      if m not in other._members:
        return False
    return True

  def check(obj: Object):
    t = get_type(obj)
    return self.is_satisfied_by(t) |\label{ex:type:check}|
\end{lstlisting}


\subsection{Optimization}
\label{ssec:optimization}

There are two aspects to our implementation that are critical for a minimal-overhead solution:

\begin{itemize}
  \item specialized executions of the type checking node, along with guards to protect these specialized versions, and
  \item a matrix to cache sub-typing relationships to eliminate
    redundant exhaustive subtype tests.
\end{itemize}
 
%Here we discuss each of the aspects in more detail.

\begin{lstlisting}[label={ex:typenode},escapechar=|,caption={An illustration of the type checking node that support type checking},float,floatplacement=htbp,columns=flexible,morekeywords={global}]
global record: Matrix

class TypeCheckNode(Node):

  expected: Type

  @Spec(static_guard=expected.check(obj))
  def check(obj: Number): |\label{ex:typenode:number}|
    pass

  @Spec(static_guard=expected.check(obj))
  def check(obj: String): |\label{ex:typenode:string}|
    pass

  ...

  @Spec(
      guard=obj.shape==cached_shape,
      static_guard=expected.check(obj))
  def check(obj: Object, @Cached(obj.shape) cached_shape: Shape): |\label{ex:typenode:object}|
    pass
  
  @Fallback
  def check_generic(obj: Any): |\label{ex:typenode:generic}|
    T = get_type(obj)
    
    if record[T, expected] is unknown: |\label{ex:typenode:matrix}|
      record[T, expected] =
          T.is_subtype_of(expected) |\label{ex:typenode:reuse}|

    if not record[T, expected]:
      raise TypeError(
          "{obj} doesn't implement {expected}")
\end{lstlisting}

The first performance-critical aspect to our implementation
is the optimization of the type checking node.
We rely on Truffle and its TruffleDSL\citep{humer2014domainspecific}.
This means we provide a number of special cases,
which are selected during execution based on the observed concrete 
kinds of objects.
A sketch of our type checking node using a pseudo-code version of the DSL
is given in \cref{ex:typenode}.
A simple optimization is for well known types such as
numbers (\cref{ex:typenode:number}) or strings (\cref{ex:typenode:string}).
The methods annotated with \code{@Spec} (shorthand for \code{@Specialization})
correspond to possible states in a state machine that is generated by the
TruffleDSL.
Thus, if a check node observes a number or a string,
it will check on the first execution only that the expected type,
\ie, the one defined by some type annotation,
is satisfied by the object by using a \code{static\_guard}.
If this is the case, the DSL will activate this state.
For just-in-time compilation, only the activated states and their normal guards are considered.
A \code{static\_guard} is not included in the optimized code.
If a check fails, or no specialization matches, the fallback,
\ie, \code{check\_generic} is selected (\cref{ex:typenode:generic}),
which may raise a type error.

For generic objects, we rely on the specialization on \cref{ex:typenode:object},
which checks that the object satisfies the expected type.
If that is the case, it reads the shape of the object (cf. \cref{ssec:moth}) at specialization time,
and caches it for later comparisons.
Thus, during normal execution,
we only need to read the shape of the object and then compare it to the cached shape
with a simple reference comparison.
If the shapes are the same, we can assume the type check passed successfully.
Note that shapes are not equivalent to types,
however, shapes imply the set of members of an object, and thus,
do imply whether an object fulfills one of our structural types.

The other performance-critical aspect to our implementation
is the use of a matrix to cache sub-typing relationships.
The matrix compares types against types,
featuring all known types along the columns and the same types again along the rows.
A cell in the table corresponds to a sub-typing relationship:
does the type corresponding to the row implement
the type corresponding to the column?
All cells in the matrix begin as unknown and, as
encountered in checks during execution, we populate the table.
If a particular relationship has been computed before
we can skip the check and instead recall the previously-computed value
(\cref{ex:typenode:reuse}).
Using this table we are able to eliminate the redundancy of evaluating
the same type to type relationships across different checks in the program. To reduce redundancy further we also unify types in a similar way to Java's string interning; 
during the construction of a type we first check to see if the same
set of members is expressed by a previously-created type and, if so,
we avoid creating the new instance and provide the existing one instead.


Together the self-specializing type check node and the cache matrix 
ensure that our implementation eliminates redundancy, and
consequently, we are able to minimize the run-time overhead of our system. 

%%let's nor worry about that now.  another whole paper at least
%% \smtodo{Yeah, then the insight here is that we can’t communicate
%%   “purity”. And generally, I think the insight with many kinds of
%%   language implementations is that these methods we may want to be
%%   “visibly pure”/idempotent rarely are provably so.
%%   \ldots Instead, the problem is avoided by being explicit about when
%%   values are valid.} 


% The Graal compiler 
% Moth needs to query the shape of arguments to evaluate them.
% Moth then needs to query the shape again to run the specializations' guards.
% Once profiled Graal can remove this redundancy by eliminating
% reusing the first query.
% During this compilation Graal also employs other optimizations 
% -- sub-expression elimination, inlining, propagation and folding --\tabularnewline
% and, as we demonstrate next in \cref{sec:evaluation},
% produces a compiled version of the method with minimal overhead incurred from
% the type check.
