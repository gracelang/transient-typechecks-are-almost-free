%!TEX root = ../latex/paper.tex

\section{Evaluation}
\label{sec:evaluation}

\newcommand{\NumIterationsAll}{1000\xspace}
\newcommand{\NumIterationsHiggs}{100\xspace}


To evaluate our approach to dynamic type checking,
we first establish the baseline performance of Moth
compared to Java and JavaScript,
and then assess the impact of the type checks themselves.

\subsection{Methodology and Setup}

To account for the complex warmup behavior
of modern system\citep{Barrett:2017:VMW},
we run each benchmark for \NumIterationsAll iterations in the same
VM invocation.\footnote{
For the Higgs VM, we only use \NumIterationsHiggs iterations,
because of it lower performance.
This is sufficient since its compilation approach induces less variation
and leads to more stable measurements.}
Afterwards, we inspected the run-time plots over the iterations
and manually determined a cutoff of \WarmupCutOff iterations for warmup,
\ie, we discard iterations with signs of compilation.
In this work, we do not consider startup performance,
because we want to assess the impact on dynamic type checks
on the best possible performance.

% Yuria
%  - Ubuntu 16.04.4, Kernel 3.13
%  - 24 hyperthreads
%  - Intel Xeon E5-2620 v3 2.40GHz
% Graal 0.33 Feb. 2018
All experiments were executed on a machine running Ubuntu Linux 16.04.4,
with Kernel 3.13.
The machine has two Intel Xeon E5-2620 v3 2.40GHz,
with 6 cores each, for a total of 24 hyperthreads.
We used ReBench 0.9.1, Java 1.8.0\_171, Graal 0.33 (\code{a13b888}),
Node.js 10.4, and Higgs from 9 May 2018 (\code{aa95240}).
Our experimental setup is available online to enable reproductions.\footnote{
\smtodo{add version info}
\url{https://gitlab.com/richard-roberts/moth-benchmarks/tree/dev}}

\sm{averages over ratios are reported as geomeans}

\subsection{\AWFY?}
\label{sec:baseline-perf}


\begin{figure}
	\AwfyBaseline{}
	\caption{Comparison of Java 1.8, Node.js 10.4, Higgs VM, and Moth.
  The boxplot depicts the peak-performance results for the \AWFY benchmarks,
  each benchmark normalized based on the result for Java.
  For these benchmarks, Moth reaches within the performance range
  of JavaScript, as implemented by Node.js,
  which makes Moth an acceptable platform for our experiments.}
	\label{fig:awfy-baseline}
\end{figure}

% - setting a base line with the AWFY benchmarks
% - comparing
%   - Java
%   - Node.js
%   - Moth
%   - Higgs
To establish the performance of Moth,
we compare it to Java and JavaScript.
For JavaScript we chose two implementations,
Node.js with V8 as well as the Higgs VM.
The Higgs VM is an interesting point of comparison,
because \citet{Richards2017} used it in their study.

We compare across languages based on the \AWFY benchmarks\citep{Marr2016},
which are designed to enable a comparison
of the effectiveness of compilers across different languages.
To this end, they use only a common set of core language elements.
While this reduces the performance relevant differences between languages,
the set of core language elements covers only common object-oriented language
features with first-class functions.
Consequently, these benchmarks are not necessarily a predictor
for application performance,
but can give a good indication for basic mechanisms such as type checking.
Furthermore, in an educational setting,
we assume that students will focus on using these basic language features
as they learn a new language.
% - arguing that Moth has state of the art performance on the given benchmarks
% - this is a reasonable foundation to make performance claims
%   that can generalize to state-of-the-art custom VMs such as V8

\Cref{fig:awfy-baseline} shows the results.
We use Java as baseline since it is the fasted language implementation
in this experiment.
We see that Node.js (V8) is about
\OverheadNodeGMeanX (min. \OverheadNodeMinX, max. \OverheadNodeMaxX)
slower than Java.
Moth is about \OverheadMothGMeanX (min. \OverheadMothMinX, max. \OverheadMothMaxX) slower than Java.
As such, its on average \OverheadMothNodeGMeanP (min. \OverheadMothNodeMinP, max. \OverheadMothNodeMaxX) slower than Node.js.
Compared to the Higgs VM, which is on these benchmarks
\OverheadHiggsGMeanX (min. \OverheadHiggsMinX, max. \OverheadHiggsMaxX) slower than Java,
Moth reaches the performance of Node.js much closer.
With these results, we argue that Moth is a suitable platform to
assess the impact of our approach to dynamic type checking,
because its performance is close enough to state-of-the-art VMs,
and run-time overhead is not going to be hidden by slow baseline performance.


\subsection{Performance of Dynamic Type Checking}

% - benchmark selection
%  - dynamic type checking performance determined based on commonly used
%    benchmarks from the gradual typing papers
% - we are subsetting, need to explain why
%   - the subset of benchmarks that we managed to port to Grace/Moth

The performance overhead of our dynamic type checking system
is assessed based on the \AWFY benchmarks
as well as benchmarks from the gradual-typing literature.
Our goal was to use benchmarks that are widely used and can be ported to Grace.
To this end, we surveyed a number of papers\citep{Takikawa2016,Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Stulova2016,Greenman2018}
and selected the benchmarks that have been used by multiple papers.
Some of the benchmarks overlapped with the \AWFY suite,
or were available in different versions.
While not always behavioral equivalent,
we used the \AWFY benchmarks since we already used them to
establish the performance baseline.

\begin{table}
\caption{Benchmarks selected from literature.}
\label{tab:gradual-benchmarks}
\begin{tabular}{l l r}
Fannkuch & \cite{Vitousek2017,Greenman2018} \\
Float & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
Go & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
NBody & \cite{Kuhlenschmidt:2018:preprint,Vitousek2017,Greenman2018} & used \cite{Marr2016} \\
Queens & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} & used \cite{Marr2016} \\
PyStone & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
Sieve & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017} & used \cite{Marr2016} \\
Snake & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017} \\
Spectralnorm & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
\end{tabular}
\end{table}

\begin{figure}
	\TypingOverhead{}
	\caption{Overhead\smtodo{describe}}
	\label{fig:typing-overhead}
\end{figure}

\begin{cnote}
  - which experiments?
  
  - fully types
  - no types
  - no types on fields
  - all run on moth

\end{cnote}

