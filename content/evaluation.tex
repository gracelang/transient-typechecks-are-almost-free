%!TEX root = ../latex/paper.tex

\section{Evaluation}
\label{sec:evaluation}


To evaluate our approach to dynamic type checking,
we first establish the baseline performance of Moth
compared to Java and JavaScript,
and then assess the impact of the type checks themselves.

\subsection{Methodology and Setup}

To account for the complex warmup behavior
of modern system\citep{Barrett:2017:VMW},
we run each benchmark for 1000\smtodo{add macro} iterations in the same
VM invocation.\footnote{
For the Higgs VM, we only use NN\smtodo{add macro} iterations,
because of it lower performance.
This is sufficient since its compilation approach induces less variation
and leads to more stable measurements.}
Afterwards, we inspected the run-time plots over the iterations
and manually determined a cutoff of NN\smtodo{add macro} iterations for warmup,
i.e., we discard iterations with signs of compilation.
In this work, we do not consider startup performance,
because we want to assess the impact on dynamic type checks
on the best possible performance.

% Yuria
%  - Ubuntu 16.04.4, Kernel 3.13
%  - 24 hyperthreads
%  - Intel Xeon E5-2620 v3 2.40GHz
% Graal 0.33 Feb. 2018
All experiments were executed on a machine running Ubuntu Linux 16.04.4,
with Kernel 3.13.
The machine has two Intel Xeon E5-2620 v3 2.40GHz,
with 6 cores, each for a total of 24 hyperthreads.
We used ReBench 0.9.1, Java 1.8.0\_171, Graal 0.33 (\code{a13b888}),
Node.js 10.4, and Higgs from 9 May 2018 (\code{aa95240}).
Our experimental setup is available online to enable reproductions.\footnote{
\smtodo{add version info}
\url{https://gitlab.com/richard-roberts/moth-benchmarks/tree/dev}}


\subsection{\AWFY?}
\label{sec:baseline-perf}


\begin{figure}
	\AwfyBaseline{}
	\caption{Overhead\smtodo{describe}}
	\label{fig:awfy-baseline}
\end{figure}

% - setting a base line with the AWFY benchmarks
% - comparing
%   - Java
%   - Node.js
%   - Moth
%   - Higgs
To establish the performance of Moth,
we compare it to Java and JavaScript.
For JavaScript we chose two implementations,
Node.js with V8 as well as the Higgs VM.
The Higgs VM is an interesting point of comparison,
because \citet{Richards2017} used it in their study.

We compare across languages based on the \AWFY benchmarks\citep{Marr2016},
which are designed to enable a comparison
of the effectiveness of compilers across different languages.
To this end, they use only a common set of core language elements.
While this reduces the performance relevant differences between languages,
the set of core language elements covers only common object-oriented language
features with first-class functions.
Consequently, these benchmarks are not necessarily a predictor
for application performance,
but can give a good indication for basic mechanisms such as type checking.

\Cref{fig:awfy-baseline} shows the results.
We use Java as baseline since it is the fasted language implementation
in this experiment.
We see that Node.js (V8) is about XXx\smtodo{add macro} (min, max) slower than Java.
Moth is about XXx\smtodo{add macro} (min, max) slower than Java.
As such, its on average XXx\smtodo{add macro} slower than Node.js.
Compared to the Higgs VM, which is on these benchmarks
XXx\smtodo{add macro} (min, max) than Java,
Moth reaches the performance of Node.js much closer.
With these results, we argue that Moth is a suitable platform to
assess the impact of our approach to dynamic type checking,
because its performance is close enough to state-of-the-art VMs,
and run-time overhead is not going to be hidden by slow baseline performance.

\begin{cnote}
- arguing that Moth has state of the art performance on the given benchmarks
- this is a reasonable foundation to make performance claims
  that can generalize to state-of-the-art custom VMs such as V8
\end{cnote}

\subsection{Performance of Dynamic Type Checking}

The performance overhead of our dynamic type checking system
is assessed based on the \AWFY benchmarks
as well as benchmarks from the gradual-typing literature.
Our goal was to use benchmarks that are widely used and can be ported to Grace.
To this end, we surveyed a number of papers\citep{Takikawa2016,Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Stulova2016,Greenman2018}
and selected the benchmarks that have been used by multiple papers.
Some of the benchmarks overlapped with the \AWFY suite,
or were available in different versions.
While not always behavioral equivalent,
we used the \AWFY benchmarks since we used them to
establish the performance baseline, too.

\begin{table}
\caption{Benchmarks selected from literature.}
\label{tab:gradual-benchmarks}
\begin{tabular}{l l r}
Fannkuch & \cite{Vitousek2017,Greenman2018} \\
Float & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
Go & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
NBody & \cite{Kuhlenschmidt:2018:preprint,Vitousek2017,Greenman2018} & used \cite{Marr2016} \\
Queens & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} & used \cite{Marr2016} \\
PyStone & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
Sieve & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017} & used \cite{Marr2016} \\
Snake & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017} \\
Spectralnorm & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
\end{tabular}
\end{table}

\begin{figure}
	\TypingOverhead{}
	\caption{Overhead\smtodo{describe}}
	\label{fig:typing-overhead}
\end{figure}

\begin{cnote}
- benchmark selection
 - dynamic type checking performance determined based on commonly used
   benchmarks from the gradual typing papers
   - name the benchmarks
   - for each, indicate which paper used them
   - we are subsetting, need to explain why
     - the subset of benchmarks that we managed to port to Grace/Moth


  - which experiments?
  
  - fully types
  - no types
  - no types on fields
  - all run on moth

\end{cnote}

