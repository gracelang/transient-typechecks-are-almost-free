%!TEX root = ../latex/paper.tex

\section{Evaluation}
\label{sec:evaluation}

\newcommand{\NumIterationsAll}{1000\xspace}
\newcommand{\NumIterationsHiggs}{100\xspace}


To evaluate our approach to dynamic type checking,
we first establish the baseline performance of Moth
compared to Java and JavaScript,
and then assess the impact of the type checks themselves.

\subsection{Methodology and Setup}

To account for the complex warmup behavior
of modern system\citep{Barrett:2017:VMW},
we run each benchmark for \NumIterationsAll iterations in the same
VM invocation.\footnote{
For the Higgs VM, we only use \NumIterationsHiggs iterations,
because of it lower performance.
This is sufficient since its compilation approach induces less variation
and leads to more stable measurements.}
Afterwards, we inspected the run-time plots over the iterations
and manually determined a cutoff of \WarmupCutOff iterations for warmup,
\ie, we discard iterations with signs of compilation.
In this work, we do not consider startup performance,
because we want to assess the impact on dynamic type checks
on the best possible performance.

% Yuria
%  - Ubuntu 16.04.4, Kernel 3.13
%  - 24 hyperthreads
%  - Intel Xeon E5-2620 v3 2.40GHz
% Graal 0.33 Feb. 2018
All experiments were executed on a machine running Ubuntu Linux 16.04.4,
with Kernel 3.13.
The machine has two Intel Xeon E5-2620 v3 2.40GHz,
with 6 cores each, for a total of 24 hyperthreads.
We used ReBench 0.9.1, Java 1.8.0\_171, Graal 0.33 (\code{a13b888}),
Node.js 10.4, and Higgs from 9 May 2018 (\code{aa95240}).
Our experimental setup is available online to enable reproductions.\footnote{
\smtodo{add version info}
\url{https://gitlab.com/richard-roberts/moth-benchmarks/tree/dev}}

\sm{averages over ratios are reported as geomeans}

\subsection{\AWFY?}
\label{sec:baseline-perf}


\begin{figure}
	\AwfyBaseline{}
	\caption{Comparison of Java 1.8, Node.js 10.4, Higgs VM, and Moth.
  The boxplot depicts the peak-performance results for the \AWFY benchmarks,
  each benchmark normalized based on the result for Java.
  For these benchmarks, Moth is within the performance range
  of JavaScript, as implemented by Node.js,
  which makes Moth an acceptable platform for our experiments.}
	\label{fig:awfy-baseline}
\end{figure}

% - setting a base line with the AWFY benchmarks
% - comparing
%   - Java
%   - Node.js
%   - Moth
%   - Higgs
To establish the performance of Moth,
we compare it to Java and JavaScript.
For JavaScript we chose two implementations,
Node.js with V8 as well as the Higgs VM.
The Higgs VM is an interesting point of comparison,
because \citet{Richards2017} used it in their study.

We compare across languages based on the \AWFY benchmarks\citep{Marr2016},
which are designed to enable a comparison
of the effectiveness of compilers across different languages.
To this end, they use only a common set of core language elements.
While this reduces the performance relevant differences between languages,
the set of core language elements covers only common object-oriented language
features with first-class functions.
Consequently, these benchmarks are not necessarily a predictor
for application performance,
but can give a good indication for basic mechanisms such as type checking.
Furthermore, in an educational setting,
we assume that students will focus on using these basic language features
as they learn a new language.
% - arguing that Moth has state of the art performance on the given benchmarks
% - this is a reasonable foundation to make performance claims
%   that can generalize to state-of-the-art custom VMs such as V8

\Cref{fig:awfy-baseline} shows the results.
We use Java as baseline since it is the fasted language implementation
in this experiment.
We see that Node.js (V8) is about
\OverheadNodeGMeanX (min. \OverheadNodeMinX, max. \OverheadNodeMaxX)
slower than Java.
Moth is about \OverheadMothGMeanX (min. \OverheadMothMinX, max. \OverheadMothMaxX) slower than Java.
As such, its on average \OverheadMothNodeGMeanP (min. \OverheadMothNodeMinP, max. \OverheadMothNodeMaxX) slower than Node.js.
Compared to the Higgs VM, which is on these benchmarks
\OverheadHiggsGMeanX (min. \OverheadHiggsMinX, max. \OverheadHiggsMaxX) slower than Java,
Moth reaches the performance of Node.js more closely.
With these results, we argue that Moth is a suitable platform to
assess the impact of our approach to dynamic type checking,
because its performance is close enough to state-of-the-art VMs,
and run-time overhead is not hidden by slow baseline performance.


\subsection{Performance of Dynamic Type Checking}

% - benchmark selection
%  - dynamic type checking performance determined based on commonly used
%    benchmarks from the gradual typing papers
% - we are subsetting, need to explain why
%   - the subset of benchmarks that we managed to port to Grace/Moth

The performance overhead of our dynamic type checking system
is assessed based on the \AWFY benchmarks
as well as benchmarks from the gradual-typing literature.
The goal was to complement our benchmarks with additional ones that are
used for similar experiments and can be ported to Grace.
To this end, we surveyed a number of papers\citep{Takikawa2016,Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Stulova2016,Greenman2018}
and selected benchmarks that have been used by multiple papers.
Some of these benchmarks overlapped with the \AWFY suite,
or were available in different versions.
While not always behaviorally equivalent,
we chose the \AWFY versions since we already used them to
establish the performance baseline.
The list of selected benchmarks is given in \cref{tab:gradual-benchmarks}.

The benchmarks were modified to have complete type information.
We modified Moth to report absent type information and ensured it was complete.
To assess the performance overhead of type checking,
we compare the execution of Moth with all checks disabled, \ie, the baseline version from 
\cref{sec:baseline-perf}, against an execution that has all checks enabled.


\begin{table}
\caption{Benchmarks selected from literature.}
\label{tab:gradual-benchmarks}
\begin{tabular}{l l r}
Fannkuch & \cite{Vitousek2017,Greenman2018} \\
Float & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
Go & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
NBody & \cite{Kuhlenschmidt:2018:preprint,Vitousek2017,Greenman2018} & used \cite{Marr2016} \\
Queens & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} & used \cite{Marr2016} \\
PyStone & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
Sieve & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017} & used \cite{Marr2016} \\
Snake & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017} \\
SpectralNorm & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
\end{tabular}
\end{table}

\begin{figure}
	\TypingOverhead{}
	\caption{A boxplot comparing the performance of Moth without type checking
  to Moth with type checking.
  The plot depicts the run-time overhead on peak performance over
  the untyped performance. On average, dynamic type checking introduces
  an overhead of \OverheadTypingGMeanP (min. \OverheadTypingMinP, max. \OverheadTypingMaxP).}
	\label{fig:typing-overhead}
\end{figure}

The results are depicted in \cref{fig:typing-overhead}.
Overall, we see a peak-performance overhead of 
\OverheadTypingGMeanP (min. \OverheadTypingMinP, max. \OverheadTypingMaxP).

% explain maxima

The benchmark with the highest overhead of \OverheadListP is List,
which traverses a linked list and has to check the list elements individually
in a way that introduces checks that do not coincide with any shape checks
on the relevant objects that are performed in the unchecked version.
We consider this benchmark a pathological case and discuss it
in detail in \cref{sec:disc-pathological-case}.

Beside List, the highest overheads are on
Richards (\OverheadRichardsP), CD (\OverheadCDP), 
Snake (\OverheadSnakeP), and Towers (\OverheadTowersP).
Richards has one major component, also a linked list traversal,
similar to List.
Snake and Towers access primarily arrays in a way that introduces checks
that do not coincide with behavior in the unchecked version.

% explain minima

However, in some benchmarks the run time decreased; notably Permute (\OverheadPermuteP),
GraphSearch (\OverheadGraphSearchP), and Storage (\OverheadStorageP).
Permute simply creates the permutations of an array.
GraphSearch implements a page rank algorithm
and thus is primarily graph traversal.
Storage stresses the garbage collector by constructing a tree of arrays.
For these benchmarks the introduced checks seem to coincide with shape-check operations
already performed in the untyped version.
The performance improvement is possibly caused by having checks earlier,
which enables the compiler to more aggressively move them out of loops.
Another reason could simply be that the extra checks shift the boundaries
of compilation units.
In such cases, checks might not be eliminated completely,
but the shifted boundary between compilation units might mean that
objects do not need to be materialized and thus do not need to be allocated,
or simply that the generated native code interacts better with
the instruction cache of the processor.
Such shifts in performance of about 10\%
are somewhat common for highly optimizing just-in-time compilers.


% discuss optimization techniques?
% discuss specializations?


% \begin{cnote}
%   - which experiments?
%
%   - fully types
%   - no types
%   - no types on fields
%   - all run on moth
%
% \end{cnote}

