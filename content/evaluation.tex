%!TEX root = ../latex/paper.tex

\section{Evaluation}
\label{sec:evaluation}


To evaluate our approach to dynamic type checking,
we first establish the baseline performance of Moth
compared to Java and JavaScript,
and then assess the impact of the type checks themselves.

\subsection{Methodology and Setup}

To account for the complex warmup behavior
of modern system\citep{Barrett:2017:VMW},
we run each benchmark for 1000\smtodo{add macro} iterations in the same
VM invocation.\footnote{
For the Higgs VM, we only use NN\smtodo{add macro} iterations,
because of it lower performance.
This is sufficient since its compilation approach induces less variation
and leads to more stable measurements.}
Afterwards, we inspected the run-time plots over the iterations
and manually determined a cutoff of NN\smtodo{add macro} iterations for warmup,
i.e., we discard iterations with signs of compilation.
In this work, we do not consider startup performance,
because we want to assess the impact on dynamic type checks
on the best possible performance.

% Yuria
%  - Ubuntu 16.04.4, Kernel 3.13
%  - 24 hyperthreads
%  - Intel Xeon E5-2620 v3 2.40GHz
% Graal 0.33 Feb. 2018
All experiments were executed on a machine running Ubuntu Linux 16.04.4,
with Kernel 3.13.
The machine has two Intel Xeon E5-2620 v3 2.40GHz,
with 6 cores, each for a total of 24 hyperthreads.
We used ReBench 0.9.1, Java 1.8.0\_171, Graal 0.33 (\code{a13b888}),
Node.js 10.4, and Higgs from 9 May 2018 (\code{aa95240}).
Our experimental setup is available online to enable reproductions.\footnote{
\smtodo{add version info}
\url{https://gitlab.com/richard-roberts/moth-benchmarks/tree/dev}}


\subsection{Are we fast yet?}
\label{sec:baseline-perf}


\begin{figure}
	\AwfyBaseline{}
	\caption{Overhead\smtodo{describe}}
	\label{fig:awfy-baseline}
\end{figure}


To establish the performance of Moth,
we compare it 
\citep{Marr2016}

\begin{cnote}
- setting a base line with the AWFY benchmarks
- comparing
  - Java
  - Node.js
  - Moth
  - Higgs
- arguing that Moth has state of the art performance on the given benchmarks
- this is a reasonable foundation to make performance claims
  that can generalize to state-of-the-art custom VMs such as V8
\end{cnote}

\subsection{Performance of Dynamic Type Checking}

The performance overhead of our dynamic type checking system
is assessed based on the Are\,We\,Fast\,Yet benchmarks
as well as benchmarks from the gradual-typing literature.
Our goal was to use benchmarks that are widely used and can be ported to Grace.
To this end, we surveyed a number of papers\citep{Takikawa2016,Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Stulova2016,Greenman2017}
and selected the benchmarks that have been used by multiple papers.
Some of the benchmarks overlapped with the Are We Fast Yet suite,
or were available in different versions.
While not always behavioral equivalent,
we used the Are\,We\,Fast\,Yet benchmarks since we used them to
establish the performance baseline, too.

\begin{table}
\caption{Benchmarks selected from literature.}
\label{tab:gradual-benchmarks}
\begin{tabular}{l l r}
sieve of eratosthenes &
 \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017} & used \cite{Marr2016} \\
snake & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017} \\
n queens & \cite{Vitousek2017,Muehlboeck2017,Greenman2017} & used \cite{Marr2016} \\
fannkuch & \cite{Vitousek2017,Greenman2017} \\
n body & \cite{Kuhlenschmidt:2018:preprint,Vitousek2017,Greenman2017} & used \cite{Marr2016} \\
pystone & \cite{Vitousek2017,Muehlboeck2017,Greenman2017} \\
float & \cite{Vitousek2017,Muehlboeck2017,Greenman2017} \\
go & \cite{Vitousek2017,Muehlboeck2017,Greenman2017} \\
spectralnorm & \cite{Vitousek2017,Muehlboeck2017,Greenman2017}
\end{tabular}
\end{table}

\begin{figure}
	\TypingOverhead{}
	\caption{Overhead\smtodo{describe}}
	\label{fig:typing-overhead}
\end{figure}

\begin{cnote}
- benchmark selection
 - dynamic type checking performance determined based on commonly used
   benchmarks from the gradual typing papers
   - name the benchmarks
   - for each, indicate which paper used them
   - we are subsetting, need to explain why
     - the subset of benchmarks that we managed to port to Grace/Moth


  - which experiments?
  
  - fully types
  - no types
  - no types on fields
  - all run on moth

\end{cnote}

