%!TEX root = ../latex/paper.tex

\section{Evaluation}
\label{sec:evaluation}

\newcommand{\NumIterationsAll}{1000\xspace}
\newcommand{\NumIterationsHiggs}{100\xspace}


To evaluate our approach to dynamic type checking,
we first establish the baseline performance of Moth
compared to Java and JavaScript,
and then assess the impact of the type checks themselves.

\subsection{Method and Setup}

To account for the complex warmup behavior
of modern systems\citep{Barrett:2017:VMW} as well as
the non-determinism caused by \eg garbage collection and cache effects,
we run each benchmark for \NumIterationsAll iterations in the same
VM invocation.\footnote{
For the Higgs VM, we only use \NumIterationsHiggs iterations,
because of its lower performance.
This is sufficient since its compilation approach induces less variation
and leads to more stable measurements.}
Afterwards, we inspected the run-time plots over the iterations
and manually determined a cutoff of \WarmupCutOff iterations for warmup,
\ie, we discard iterations with signs of compilation.
As a result, we use a large number of data points to compute the average,
but outliers, caused by \eg garbage collection, remain visible in the plots.
In this work, we do not consider startup performance,
because we want to assess the impact of dynamic type checks
on the best possible performance.
All reported averages use the geometric mean since they aggregate ratios.

% Yuria
%  - Ubuntu 16.04.4, Kernel 3.13
%  - 24 hyperthreads
%  - Intel Xeon E5-2620 v3 2.40GHz
% Graal 0.33 Feb. 2018
All experiments were executed on a machine running Ubuntu Linux 16.04.4,
with Kernel 3.13.
The machine has two Intel Xeon E5-2620 v3 2.40GHz,
with 6 cores each, for a total of 24 hyperthreads.
We used ReBench 0.9.1, Java 1.8.0\_171, Graal 0.33 (\code{a13b888}),
Node.js 10.4, and Higgs from 9 May 2018 (\code{aa95240}).
Benchmarks were executed one by one to avoid interference between them.
Our experimental setup is available online to enable reproductions.\footnote{
%% \smtodo{add version info}
% TODO: for final version add version info
\url{https://gitlab.com/richard-roberts/moth-benchmarks/tree/dev}}


\subsection{\AWFY?}
\label{sec:baseline-perf}


\begin{figure}
	\AwfyBaseline{}
	\caption{Comparison of Java 1.8, Node.js 10.4, Higgs VM, and Moth.
  The boxplot depicts the peak-performance results for the \AWFY benchmarks,
  each benchmark normalized based on the result for Java.
  For these benchmarks, Moth is within the performance range
  of JavaScript, as implemented by Node.js,
  which makes Moth an acceptable platform for our experiments.}
	\label{fig:awfy-baseline}
\end{figure}

% - setting a base line with the AWFY benchmarks
% - comparing
%   - Java
%   - Node.js
%   - Moth
%   - Higgs
To establish the performance of Moth,
we compare it to Java and JavaScript.
For JavaScript we chose two implementations,
Node.js with V8 as well as the Higgs VM.
The Higgs VM is an interesting point of comparison,
because \citet{Richards2017} used it in their study.

We compare across languages based on the \AWFY benchmarks\citep{Marr2016},
which are designed to enable a comparison
of the effectiveness of compilers across different languages.
To this end, they use only a common set of core language elements.
While this reduces the performance relevant differences between languages,
the set of core language elements covers only common object-oriented language
features with first-class functions.
Consequently, these benchmarks are not necessarily a predictor
for application performance,
but can give a good indication for basic mechanisms such as type checking.
Furthermore, in an educational setting,
we assume that students will focus on using these basic language features
as they learn a new language.
% - arguing that Moth has state of the art performance on the given benchmarks
% - this is a reasonable foundation to make performance claims
%   that can generalize to state-of-the-art custom VMs such as V8

\Cref{fig:awfy-baseline} shows the results.
We use Java as baseline since it is the fastest language implementation
in this experiment.
We see that Node.js (V8) is about
\OverheadNodeGMeanX (min. \OverheadNodeMinX, max. \OverheadNodeMaxX)
slower than Java.
Moth is about \OverheadMothGMeanX (min. \OverheadMothMinX, max. \OverheadMothMaxX) slower than Java.
As such, its on average \OverheadMothNodeGMeanP (min. \OverheadMothNodeMinP, max. \OverheadMothNodeMaxX) slower than Node.js.
Compared to the Higgs VM, which is on these benchmarks
\OverheadHiggsGMeanX (min. \OverheadHiggsMinX, max. \OverheadHiggsMaxX) slower than Java,
Moth reaches the performance of Node.js more closely.
With these results, we argue that Moth is a suitable platform to
assess the impact of our approach to dynamic type checking,
because its performance is close enough to state-of-the-art VMs,
and run-time overhead is not hidden by slow baseline performance.


\subsection{Performance of Dynamic Type Checking}

% - benchmark selection
%  - dynamic type checking performance determined based on commonly used
%    benchmarks from the gradual typing papers
% - we are subsetting, need to explain why
%   - the subset of benchmarks that we managed to port to Grace/Moth

The performance overhead of our dynamic type checking system
is assessed based on the \AWFY benchmarks
as well as benchmarks from the gradual-typing literature.
The goal was to complement our benchmarks with additional ones that are
used for similar experiments and can be ported to Grace.
To this end, we surveyed a number of papers\citep{Takikawa2016,Vitousek2017,Muehlboeck2017,Bauman2017,Richards2017,Stulova2016,Greenman2018}
and selected benchmarks that have been used by multiple papers.
Some of these benchmarks overlapped with the \AWFY suite,
or were available in different versions.
While not always behaviorally equivalent,
we chose the \AWFY versions since we already used them to
establish the performance baseline.
The list of selected benchmarks is given in \cref{tab:gradual-benchmarks}.

The benchmarks were modified to have complete type information.
We modified Moth to report absent type information and ensured it was complete.
To assess the performance overhead of type checking,
we compare the execution of Moth with all checks disabled, \ie, the baseline version from 
\cref{sec:baseline-perf}, against an execution that has all checks enabled.


\begin{table}
\caption{Benchmarks selected from literature.}
\label{tab:gradual-benchmarks}
\begin{tabular}{l l r}
Fannkuch & \cite{Vitousek2017,Greenman2018} \\
Float & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
Go & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
NBody & \cite{Kuhlenschmidt:2018:preprint,Vitousek2017,Greenman2018} & used \cite{Marr2016} \\
Queens & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} & used \cite{Marr2016} \\
PyStone & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
Sieve & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017} & used \cite{Marr2016} \\
Snake & \cite{Takikawa2016,Muehlboeck2017,Bauman2017,Richards2017} \\
SpectralNorm & \cite{Vitousek2017,Muehlboeck2017,Greenman2018} \\
\end{tabular}
\end{table}

\begin{figure}
	\TypingOverhead{}
	\caption{A boxplot comparing the performance of Moth without type checking
  to Moth with type checking.
  The plot depicts the run-time overhead on peak performance over
  the untyped performance. On average, dynamic type checking introduces
  an overhead of \OverheadTypingGMeanP (min. \OverheadTypingMinP, max. \OverheadTypingMaxP).
  The visible outliers reflect the noise in today's complex system
  and correspond \eg to garbage collection and cache effects.}
	\label{fig:typing-overhead}
\end{figure}

The results are depicted in \cref{fig:typing-overhead}.
Overall, we see an average peak-performance overhead of 
\OverheadTypingGMeanP (min. \OverheadTypingMinP, max. \OverheadTypingMaxP).

% explain maxima

The benchmark with the highest overhead of \OverheadListP is List,
which traverses a linked list and has to check the list elements individually
in a way that introduces checks that do not coincide with any shape checks
on the relevant objects that are performed in the unchecked version.
We consider this benchmark a pathological case and discuss it
in detail in \cref{sec:disc-pathological-case}.

Beside List, the highest overheads are on
Richards (\OverheadRichardsP), CD (\OverheadCDP), 
Snake (\OverheadSnakeP), and Towers (\OverheadTowersP).
Richards has one major component, also a linked list traversal,
similar to List.
Snake and Towers primarily access  arrays in a way that introduces checks
that do not coincide with behavior in the unchecked version.

% explain minima

However, in some benchmarks the run time decreased; notably Permute (\OverheadPermuteP),
GraphSearch (\OverheadGraphSearchP), and Storage (\OverheadStorageP).
Permute simply creates the permutations of an array.
GraphSearch implements a page rank algorithm
and thus is primarily graph traversal.
Storage stresses the garbage collector by constructing a tree of arrays.
For these benchmarks the introduced checks seem to coincide with shape-check operations
already performed in the untyped version.
The performance improvement is possibly caused by having checks earlier,
which enables the compiler to more aggressively move them out of loops.
Another reason could simply be that the extra checks shift the boundaries
of compilation units.
In such cases, checks might not be eliminated completely,
but the shifted boundary between compilation units might mean that
objects do not need to be materialized and thus do not need to be allocated,
or simply that the generated native code interacts better with
the instruction cache of the processor.
% Such shifts in performance of about 10\%
% are somewhat common for highly optimizing just-in-time compilers.\mwh{cite?}
% \kjx{unfortuantely all the best citations give more detailed
  % benchmarking methodologies (Georges, Kalibera) that we don't follow.}

\begin{figure}
	\TypingWarmup{}
	\caption{Plot of the run time for the first 100 iterations.
           The gray line is a local polynomial regression fit
           with a 0.95 confidence interval
           indicating the trend.
           The first iteration, \ie, mostly interpreted, seems
           to be affected significantly only for Mandelbrot.}
	\label{fig:typing-warmup}
\end{figure}

While we did not focus on the warmup performance,
and are mainly interested in peak performance,
\cref{fig:typing-warmup} shows the first 100 iterations for each benchmark.
The run time factor is the result for the typed version over the untyped one.
Thus, any increase indicates a slow down because of typing.
The gray line indicates a smoothed version of the curve
based on local polynomial regression fitting\citep{Cleveland:1992}
using neighboring data points.
It also indicates a 0.95 confidence interval.

Focusing on the first iteration,
where we assume that most code is executed in the interpreter,
the overhead appears minimal.
Only the Mandelbrot benchmark shows a noticeable slowdown.
However, benchmarks such as Float, PyStone, and GraphSearch show various spikes.
Since spikes appear in both directions (speedups and slowdowns),
we assume that this merely indicates a shift
for instance of garbage collection pauses.
This can happen because of different heap configurations
possibly triggered by the additional data structures for type information.

% discuss optimization techniques?
% discuss specializations?


% \begin{cnote}
%   - which experiments?
%
%   - fully types
%   - no types
%   - no types on fields
%   - all run on moth
%
% \end{cnote}

\subsection{Changes to Moth}

Outlined earlier in \cref{sec:method}, a secondary
goal of our design was to enable the implementation of our approach to be
realized with few changes to the underlying interpreter.
This helps to ensure that each Grace implementation
can provide type checking in a uniform way.

By examining the history of changes maintained by our version control, 
we estimate that our implementation for Moth required
549 new lines and 59 changes to existing lines. 
The changes correspond to the implementation of 
new modules for the type class (179 lines) and 
the self-specializing type checking node (139 lines),
modifications to the front end to extract typing information
(115 new lines, 14 lines changes)
and finally the new fields and amended constructors for AST nodes 
(116 new lines, 45 lines changes).

% New Classes
% -----------
% TypeCheckNode               (139 new lines)
% SomStructuralType           (179 new lines) = 318 new

% Parsing
% -------
% returnType                  ( 18 new lines)  
% typeFor                     ( 31 new lines)
% typesForParameters          ( 27 new lines)
% typesForLocals              ( 11 new lines)
% parseTypeLiteral            ( 19 new lines)
% invoke parseTypeLiteral     (  6 new lines)
% adding return type          (  3 new lines)
% types for declarations      (  4 changed lines)
% adding type to parameters   (  4 changed lines)
% adding type locals          (  4 changed lines)
% adding type to variable     (  2 changed lines) = 115 new, 14 changed

% AST
% ---
% Storage location            ( 20 changed lines)
% local variable read/write   ( 28 new lines,  1 changed)
% add type to slot read/write (  4 new lines,  2 changed)
% cached TX slot read/write   ( 14 new lines, 21 changed)
% dispatch                    ( 31 new lines,  1 changed)
% class factory getTyoe       ( 39 new lines)     = 116 new, 45 changed

