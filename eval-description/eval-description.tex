\documentclass[a4paper,USenglish]{darts-v2019}
\usepackage{microtype}%if unwanted, comment out or use option "draft"
\usepackage{xspace}
\usepackage{cleveref}

\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Performance Evaluation: A Guide to the R Scripts and Results}

\author{Richard Roberts}{School of Design, Victoria University of Wellington}{rykardo.r@gmail.com}{https://orcid.org/0000-0002-3462-8539}{}

\author{Stefan Marr}{School of Computing, University of Kent}{s.marr@kent.ac.uk}{https://orcid.org0000-0001-9059-5180}{}

\author{Michael Homer}{School of Engineering and Computer Science, Victoria University of Wellington}{mwh@ecs.vuw.ac.nz}{}{}

\author{James Noble}{School of Engineering and Computer Science, Victoria University of Wellington}{kjx@ecs.vuw.ac.nz}{https://orcid.org/0000-0001-9036-5692}{}

\authorrunning{Roberts, Marr, Homer, Noble}

\Copyright{Richard Roberts, Stefan Marr, Michael Homer, James Noble}

\ccsdesc[500]{Software and its engineering~Just-in-time compilers}
\ccsdesc[300]{Software and its engineering~Object oriented languages}
\ccsdesc[300]{Software and its engineering~Interpreters}

\keywords{dynamic type checking, gradual types, optional types, Grace,
Moth, object-oriented programming}

\input{../evaluation/scripts/knitr-init.tex}
\input{generated/typing.tex}
\input{generated/type-stats.tex}
\input{generated/awfy.tex}
\input{generated/optimizations.tex}
\input{generated/type-cost.tex}

\def\SOMns{SOM{\sc ns}\xspace}
\def\AWFY{Are\,We\,Fast\,Yet\xspace}
\newcommand{\ie}{i.e.\xspace}
\newcommand{\eg}{e.g.\xspace}
\newcommand{\code}[1]{\texttt{#1}}



% %TODO Please provide information to the related scholarly information
% \RelatedArticle{John Q. Open and Joan R. Access, ``A very nice paper'', in Proceedings of the 30th Conference on Very Important Topics (CVIT 2016), LIPIcs, Vol.~0, pp.~0:1--0:2, 2016.\newline \url{https://doi.org/10.4230/LIPIcs.xxx.xxx.xxx}}
%
% \acknowledgements{I want to thank \dots}%optional
%
% %Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \Volume{3}
% \Issue{2}
% \Article{1}
% \RelatedConference{42nd Conference on Very Important Topics (CVIT 2016), December 24--27, 2016, Little Whinging, United Kingdom}
% % Editor-only macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\begin{abstract}
This document gives a brief overview
over the scripts used for the performance evaluation.
The goal is to provide enough guidance
for someone with experience with R
to be able to reproduce the plots and numbers from the paper
based on our data.

This then may also provide a foundation to evaluate data from other runs,
though, this may require extra handling of the data in the scripts,
because the scripts currently are tailored for a specific set of data files.
Ideally, all data would be in a single file, as produced by \code{rebench} run
of all experiments. However, because of the long run time, this was infeasible
and the data is spread over multiple files.

The general methodology is described in the paper.
Instructions on how to setup and execute the benchmarks
are in the root folder of the artifact.
\end{abstract}

\section{Basic Setup}

The evaluation section is written using a mix of Latex files
as well as Rnw (Sweave) files to generate Latex using R and knitr.

For an overview of knitr, see for instance \url{https://yihui.name/knitr/}.
%
The Rnw files as well as some Rmd (R + markdown) files are in the
adjacent \code{evaluation} folder.

This document gives a brief overview, following the order in the paper.
%
This document itself is rendered using \code{make},
which will execute all R code, generate all required files,
and finally produce this PDF using Latex.

\section{\AWFY?}
\label{sec:baseline-perf}

The paper starts with comparing the performance of Moth
to a number of other languages to establish a performance baseline.

The file \code{awfy.Rnw} contains the calculations to generate
\cref{fig:awfy-baseline} as well as a number of macros with summary statistics.
The data is generated by \code{rebench} using the \code{steady} experiment
defined in the \code{codespeed.conf}.
Thus, the data file produced by \code{rebench codespeed.conf steady} should be useable by this script.

\begin{figure}[htb]
  \centering
	\AwfyBaseline{}
	\caption{Comparison of Java 1.8, Node.js 10.4, Higgs VM, and Moth.
  The boxplot depicts the peak-performance results for the \AWFY benchmarks,
  each benchmark normalized based on the result for Java.
  For these benchmarks, Moth is within the performance range
  of JavaScript, as implemented by Node.js,
  which makes Moth an acceptable platform for our experiments.}
	\label{fig:awfy-baseline}
\end{figure}

The script itself loads the data file,
discards data of unrelated experiments,
and disregards measurements that are consider to show compilation,
as described in the paper.

Afterwards, it processes the data to be useable for the plot.
It normalizes the data either based on the Java execution or on Node.js.
This normalization is a basic unit conversion and does not influence the
statistical properties of the data.

Eventually, it defines the \code{\textbackslash{}AwfyBaseline} macro,
which will include the plot as a tikz file into the paper.
The result is \cref{fig:awfy-baseline}.

It also generates the macros in \cref{tab:awfy},
which have the numeric values for use in the paper.

\begin{table}[htb]
\caption{\AWFY Summary Statistics for use as macros in the paper.}
\begin{tabular}{lr}

OverheadNodeGMeanX     & \OverheadNodeGMeanX \\
OverheadNodeMinX       & \OverheadNodeMinX \\
OverheadNodeMaxX       & \OverheadNodeMaxX \\
                       &  \\
OverheadMothGMeanX     & \OverheadMothGMeanX \\
OverheadMothMinX       & \OverheadMothMinX \\
OverheadMothMaxX       & \OverheadMothMaxX \\
                       &  \\
OverheadMothNodeGMeanX & \OverheadMothNodeGMeanX \\
OverheadMothNodeMinX   & \OverheadMothNodeMinX \\
OverheadMothNodeMaxX   & \OverheadMothNodeMaxX \\
                       &  \\
OverheadMothNodeGMeanP & \OverheadMothNodeGMeanP \\
OverheadMothNodeMinP   & \OverheadMothNodeMinP \\
OverheadMothNodeMaxP   & \OverheadMothNodeMaxP \\
                       &  \\
OverheadHiggsGMeanX    & \OverheadHiggsGMeanX \\
OverheadHiggsMinX      & \OverheadHiggsMinX \\
OverheadHiggsMaxX      & \OverheadHiggsMaxX \\

\end{tabular}
\label{tab:awfy}
\end{table}

\section{Performance of Transient Gradual Type Checks}

The second part of the evaluation focuses on the cost of the type checks.

The file with the analysis code is \code{typing.Rnw}.
Thus, the data file produced by \code{rebench codespeed.conf typing}.

The basic structure of the script is the same as for \code{awfy.Rnw}.
The data is loaded, and irrelevant data is discarded.
For this analysis, we only consider data after warmup
for Moth with both optimizations and Moth without types.

Then, we generate the macros \code{\textbackslash{}TypingOverhead}
and \code{\textbackslash{}TypingWarmupConfInterval}
to produce \cref{fig:typing-overhead} and \cref{fig:typing-warmup}.
The file contains a few variations, which we used to see
which visualization works best.

\begin{figure}[htb]
  \centering
	\TypingOverhead{}
	\caption{A boxplot comparing the performance of Moth
  with and without type checking.
  The plot depicts the run-time overhead on peak performance over
  the untyped performance. On average, transient type checking introduces
  an overhead of \OverheadTypingGMeanP (min. \OverheadTypingMinP, max. \OverheadTypingMaxP).
  The visible outliers reflect the noise in today's complex system
  and correspond \eg to garbage collection and cache effects.
  Note that the axis is logarithmic to avoid distorting the proportions
  of relative speedups and slowdowns.}
	\label{fig:typing-overhead}
\end{figure}

\begin{figure}[htb]
  \centering
  \TypingWarmupConfInterval{}
	\caption{Plot of the run time for the first 100 iterations.
           The lines indicate the mean at iteration $n$ normalized
           to the untyped result, the lighter area indicates a $95\%$ confidence interval.
           The first iteration, \ie, mostly interpreted, seems
           to be affected significantly only for Mandelbrot, though
           CD shows slower behavior in early warmup, too.}
	\label{fig:typing-warmup}
\end{figure}

As before, we also generate a number of macros, which are shown in \cref{tab:typing}.

\begin{table}[htb]
\caption{Typing Overhead Summary Statistics for use as macros in the paper.}
\begin{tabular}{lr}
  
OverheadTypingGMeanX   & \OverheadTypingGMeanX  \\
OverheadTypingMinX     & \OverheadTypingMinX    \\
OverheadTypingMaxX     & \OverheadTypingMaxX    \\
                       &                       \\
OverheadTypingGMeanP   & \OverheadTypingGMeanP  \\
OverheadTypingMinP     & \OverheadTypingMinP    \\
OverheadTypingMaxP     & \OverheadTypingMaxP    \\
                       &                       \\
OverheadListP          & \OverheadListP         \\
                       &                       \\
OverheadPermuteP       & \OverheadPermuteP      \\
OverheadGraphSearchP   & \OverheadGraphSearchP  \\
OverheadStorageP       & \OverheadStorageP      \\
                       &                       \\
OverheadRichardsP      & \OverheadRichardsP     \\
OverheadCDP            & \OverheadCDP           \\
OverheadSnakeP         & \OverheadSnakeP        \\
OverheadTowersP        & \OverheadTowersP      

\end{tabular}
\label{tab:typing}
\end{table}


\section{Effectiveness of Optimizations}

The third part of the evaluation assess how effective the optimizations are.

The section is generated in part from
\code{type-stats.Rnw} and \code{optimizations.Rnw}.
The data file produced by \code{rebench codespeed.conf stats}
produces the statistics on type checks,
while \code{rebench codespeed.conf typing}
is used for the performance assessment and plots.

\paragraph*{Impact on Performed Type Tests}

Similar to the other scripts,
\code{type-stats.Rnw} starts by loading the data.
However, because of the way the experiments are defined and the data is recorded
in the data files, it needs to do some extra transformations to reshape the data
for processing.
Afterwards, it takes the statistics
and generates the \code{\textbackslash{}TypingStatsTable} macro,
which is \cref{tab:type-statistics}.


\begin{table}[htb]
  \caption{Type Test Statistics over all Benchmarks.
  This table shows how many of the type tests can be avoided based on our two optimizations.
  With the use of an optimized node that replaces type checks with simple object shape checks,
  \code{check\_generic} is invoked only for the first time that a lexical location
  sees a specific object shape, which eliminates run-time type checks almost completely.
  Using our subtype matrix that caches type-check results,
  invocations of \code{is\_subtype\_of} are further reduced by an order of magnitude.}
  \label{tab:type-statistics}

  \begin{center}
    \TypingStatsTable{}
  \end{center}
\end{table}

\paragraph*{Impact on Performance}

\code{optimizations.Rnw} has the usual elements.
It loads the data, discards irrelevant data, and then calculates the
statistics. As a result, it generates the \code{\textbackslash{}OptimizationOverview} macro,
which is \cref{fig:perf-impact-optimization}.

\begin{figure}[htb]
  \centering
	\OptimizationOverview{}
  \caption{Performance Impact of the Optimizations on the Average Peak Performance over all Benchmarks.
  The boxplot shows the performance of Moth normalized to the untyped version, \ie,
  without any type checks.
  The performance of Moth with both optimizations and Moth
  with only the node for optimized type checks are identical.
  The subtype check cache improves performance over the unoptimized version,
  but does not contribute to the peak performance.
  }
	\label{fig:perf-impact-optimization}
\end{figure}


\section{Transient Typechecks are (Almost) Free}

The final part of the evaluation looks into microbenchmarks
and impact on warmup performance.

The plots are generated by \code{type-cost.Rnw}.
The data file is produced by \code{rebench codespeed.conf type-cost}.
The file first loads the data and then generates macros with the plots.
The macros that defined \cref{fig:type-cost-micro} are
\code{\textbackslash{}TypeCostFirstIt} and
\code{\textbackslash{}TypeCostLastIt}.

\begin{figure}
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \TypeCostFirstIt{}
    \caption{Iteration 1.}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \TypeCostLastIt{}
    \caption{Iteration 100.}
  \end{subfigure}

	% \TypeCost{}
  \caption{Transient Typechecks are (Almost) Free.
    Two microbenchmarks demonstrate the common scenario that the addition
    of type annotations in our system does not have an impact on peak performance.
    The two microbenchmarks are measured in 6 variants, stepwise increasing the number
    of method arguments that have type annotations.
    Furthermore, we show the result for the first benchmark iteration and the
    one hundredth.
    Moth (neither), \ie, Moth without our two optimizations sees a linear increase in run time.
    For the first iteration, we see some difference between Moth (both) and Moth (untyped).
    By the hundredth iteration, however, the compiler has eliminated
    the overhead of the type checks
    and both Moth variants essentially have the same performance
    (independent of the number of method arguments with type annotations).}
	\label{fig:type-cost-micro}
\end{figure}

\end{document}
