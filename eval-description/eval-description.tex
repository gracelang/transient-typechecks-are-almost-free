\documentclass[a4paper,USenglish]{darts-v2019}
\usepackage{microtype}%if unwanted, comment out or use option "draft"
\usepackage{xspace}
\usepackage{cleveref}

\nolinenumbers % to disable line numbers


\bibliographystyle{plainurl}% the mandatory bibstyle

\title{Performance Evaluation: A Guide to the R Scripts and Results}

\author{Richard Roberts}{School of Design, Victoria University of Wellington}{rykardo.r@gmail.com}{https://orcid.org/0000-0002-3462-8539}{}

\author{Stefan Marr}{School of Computing, University of Kent}{s.marr@kent.ac.uk}{https://orcid.org0000-0001-9059-5180}{}

\author{Michael Homer}{School of Engineering and Computer Science, Victoria University of Wellington}{mwh@ecs.vuw.ac.nz}{https://orcid.org/0000-0003-0280-6748}{}

\author{James Noble}{School of Engineering and Computer Science, Victoria University of Wellington}{kjx@ecs.vuw.ac.nz}{https://orcid.org/0000-0001-9036-5692}{}

\authorrunning{Roberts, Marr, Homer, Noble}

\Copyright{Richard Roberts, Stefan Marr, Michael Homer, James Noble}

\ccsdesc[500]{Software and its engineering~Just-in-time compilers}
\ccsdesc[300]{Software and its engineering~Object oriented languages}
\ccsdesc[300]{Software and its engineering~Interpreters}

\keywords{dynamic type checking, gradual types, optional types, Grace,
Moth, object-oriented programming}

\input{../evaluation/scripts/knitr-init.tex}
\input{generated/typing.tex}
\input{generated/type-stats.tex}
\input{generated/awfy.tex}
\input{generated/optimizations.tex}
\input{generated/type-cost.tex}

\def\SOMns{SOM{\sc ns}\xspace}
\def\AWFY{Are\,We\,Fast\,Yet\xspace}
\newcommand{\ie}{i.e.\xspace}
\newcommand{\eg}{e.g.\xspace}
\newcommand{\code}[1]{\texttt{#1}}



% %TODO Please provide information to the related scholarly information
% \RelatedArticle{John Q. Open and Joan R. Access, ``A very nice paper'', in Proceedings of the 30th Conference on Very Important Topics (CVIT 2016), LIPIcs, Vol.~0, pp.~0:1--0:2, 2016.\newline \url{https://doi.org/10.4230/LIPIcs.xxx.xxx.xxx}}
%
% \acknowledgements{I want to thank \dots}%optional
%
% %Editor-only macros:: begin (do not touch as author)%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \Volume{3}
% \Issue{2}
% \Article{1}
% \RelatedConference{42nd Conference on Very Important Topics (CVIT 2016), December 24--27, 2016, Little Whinging, United Kingdom}
% % Editor-only macros::end %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

\begin{abstract}
This document provides an overview,
following roughly the order in the paper,
of the scripts used for the performance evaluation.
The goal is to enable
someone experienced in R
to reproduce the plots and numbers from the paper
based on our data.

In addition to the above, this document serves as
a foundation to evaluate data from other runs
(doing so may require extra handling of scripts and/or the data).
\end{abstract}

\section{Preliminary Notes}

\begin{itemize}

\item The data we use for evaluation is divided into multiple files.
Ideally, all data would be in a single file,
as produced by \code{rebench} run of all experiments.
However, due to the long run time, this was infeasible.

\item Please refer to the paper for the general methodology and
to the artifact's documentation for instructions on how
to setup and execute the benchmarks.

\end{itemize}

\section{Basic Setup}

The evaluation section is written using a mix of \code{LaTeX} files
as well as \code{Rnw} (Sweave) files to generate LaTeX using R and \code{knitr}
(see \url{https://yihui.name/knitr/} For an overview of knitr).
%
The Rnw files as well as some \code{Rmd} (R + markdown) files are in the
adjacent \code{evaluation} folder.
%
This document is rendered using \code{make},
which, as part of the make process,
will execute all R code, generate all required files,
and finally produce this PDF using LaTeX.

\section{\AWFY?}
\label{sec:baseline-perf}

To establish a performance baseline,
the paper compares the performance of Moth
to a number of other languages.

The file \code{awfy.Rnw} contains the calculations to generate
\cref{fig:awfy-baseline} as well as a number of macros with summary statistics.
The data is generated by \code{rebench} using the \code{steady} experiment
defined in the \code{codespeed.conf} configuration file.

To reproduce \cref{fig:awfy-baseline}
first execute the command \code{rebench codespeed.conf steady}.
The resulting data can be rendered by \code{awfy.Rnw} to reproduce the figure:
as described in the paper
it first loads the data file
and discards data of unrelated experiments
and measurements that are considered to show compilation;
next, it processes the data to be useable for the plot
by normalizing the data (based on the Java execution or on Node.js);\footnote{
This normalization is a basic unit conversion and does not influence the
statistical properties of the data.
}
and, finally, it defines the \code{\textbackslash{}AwfyBaseline} macro,
which will include the plot as a tikz file with the paper.
The result is \cref{fig:awfy-baseline}.

\begin{figure}[htb]
  \centering
	\AwfyBaseline{}
	\caption{Comparison of Java 1.8, Node.js 10.4, Higgs VM, and Moth.
  The boxplot depicts the peak-performance results for the \AWFY benchmarks,
  each benchmark normalized based on the result for Java.
  For these benchmarks, Moth is within the performance range
  of JavaScript, as implemented by Node.js,
  which makes Moth an acceptable platform for our experiments.}
	\label{fig:awfy-baseline}
\end{figure}

In addition to the above, the \code{awfy.Rnw} script also
generates the macros in \cref{tab:awfy},
which contain the numeric values for use in the paper.

\begin{table}[htb]
\caption{\AWFY Summary Statistics for use as macros in the paper.}
\begin{tabular}{lr}

OverheadNodeGMeanX     & \OverheadNodeGMeanX \\
OverheadNodeMinX       & \OverheadNodeMinX \\
OverheadNodeMaxX       & \OverheadNodeMaxX \\
                       &  \\
OverheadMothGMeanX     & \OverheadMothGMeanX \\
OverheadMothMinX       & \OverheadMothMinX \\
OverheadMothMaxX       & \OverheadMothMaxX \\
                       &  \\
OverheadMothNodeGMeanX & \OverheadMothNodeGMeanX \\
OverheadMothNodeMinX   & \OverheadMothNodeMinX \\
OverheadMothNodeMaxX   & \OverheadMothNodeMaxX \\
                       &  \\
OverheadMothNodeGMeanP & \OverheadMothNodeGMeanP \\
OverheadMothNodeMinP   & \OverheadMothNodeMinP \\
OverheadMothNodeMaxP   & \OverheadMothNodeMaxP \\
                       &  \\
OverheadHiggsGMeanX    & \OverheadHiggsGMeanX \\
OverheadHiggsMinX      & \OverheadHiggsMinX \\
OverheadHiggsMaxX      & \OverheadHiggsMaxX \\

\end{tabular}
\label{tab:awfy}
\end{table}

\section{Performance of Transient Gradual Type Checks}

The second part of the evaluation focuses on the cost of the type checks.
Specifically, the evluation measure the performance difference between
Moth with types (both optimizations applied) and Moth without types.

For analysis of the performance difference we use
the \code{typing.Rnw} script. This script uses the data file
produced by \code{rebench codespeed.conf typing}.
The basic structure of the script is the same as for \code{awfy.Rnw}:
the data is loaded, irrelevant data is discarded
(here we consider only the data after warmup),
and finally the macros \code{\textbackslash{}TypingOverhead}
and \code{\textbackslash{}TypingWarmupConfInterval} are generated
to produce \cref{fig:typing-overhead} and \cref{fig:typing-warmup}.
Note that the file contains a few variations
that we used to see which visualization works best.

\begin{figure}[htb]
  \centering
	\TypingOverhead{}
	\caption{A boxplot comparing the performance of Moth
  with and without type checking.
  The plot depicts the run-time overhead on peak performance over
  the untyped performance. On average, transient type checking introduces
  an overhead of \OverheadTypingGMeanP (min. \OverheadTypingMinP, max. \OverheadTypingMaxP).
  The visible outliers reflect the noise in today's complex system
  and correspond \eg to garbage collection and cache effects.
  Note that the axis is logarithmic to avoid distorting the proportions
  of relative speedups and slowdowns.}
	\label{fig:typing-overhead}
\end{figure}

\begin{figure}[htb]
  \centering
  \TypingWarmupConfInterval{}
	\caption{Plot of the run time for the first 100 iterations.
           The lines indicate the mean at iteration $n$ normalized
           to the untyped result, the lighter area indicates a $95\%$ confidence interval.
           The first iteration, \ie, mostly interpreted, seems
           to be affected significantly only for Mandelbrot, though
           CD shows slower behavior in early warmup, too.}
	\label{fig:typing-warmup}
\end{figure}

As before, we also generate a number of macros, which are shown in \cref{tab:typing}.

\begin{table}[htb]
\caption{Typing Overhead Summary Statistics for use as macros in the paper.}
\begin{tabular}{lr}

OverheadTypingGMeanX   & \OverheadTypingGMeanX  \\
OverheadTypingMinX     & \OverheadTypingMinX    \\
OverheadTypingMaxX     & \OverheadTypingMaxX    \\
                       &                       \\
OverheadTypingGMeanP   & \OverheadTypingGMeanP  \\
OverheadTypingMinP     & \OverheadTypingMinP    \\
OverheadTypingMaxP     & \OverheadTypingMaxP    \\
                       &                       \\
OverheadListP          & \OverheadListP         \\
                       &                       \\
OverheadPermuteP       & \OverheadPermuteP      \\
OverheadGraphSearchP   & \OverheadGraphSearchP  \\
OverheadStorageP       & \OverheadStorageP      \\
                       &                       \\
OverheadRichardsP      & \OverheadRichardsP     \\
OverheadCDP            & \OverheadCDP           \\
OverheadSnakeP         & \OverheadSnakeP        \\
OverheadTowersP        & \OverheadTowersP

\end{tabular}
\label{tab:typing}
\end{table}


\section{Effectiveness of Optimizations}

The third part of the evaluation assesses the effectiveness
of the two optimizations that we apply to the type checking.

The figures in this section are generated using both
\code{type-stats.Rnw} and \code{optimizations.Rnw}.
The \code{rebench codespeed.conf stats} command
produces the data file of statistics for the type checks,
while the \code{rebench codespeed.conf typing} command
produces the data file for performance assessment and the plots.

\paragraph*{Impact on Performed Type Tests}

Similar to the other scripts,
\code{type-stats.Rnw} starts by loading the data.
However, in this case,
the script needs to reshape the data
before for processing
(due to the ways that the experiments are defined and the data are recorded).
After reshaping, the script takes the statistics
and generates the \code{\textbackslash{}TypingStatsTable} macro,
which creates \cref{tab:type-statistics}.

\begin{table}[htb]
  \caption{Type Test Statistics over all Benchmarks.
  This table shows how many of the type tests can be avoided based on our two optimizations.
  With the use of an optimized node that replaces type checks with simple object shape checks,
  \code{check\_generic} is invoked only for the first time that a lexical location
  sees a specific object shape, which eliminates run-time type checks almost completely.
  Using our subtype matrix that caches type-check results,
  invocations of \code{is\_subtype\_of} are further reduced by an order of magnitude.}
  \label{tab:type-statistics}

  \begin{center}
    \TypingStatsTable{}
  \end{center}
\end{table}

\paragraph*{Impact on Performance}

\code{optimizations.Rnw} follows the usual process:
it loads the data,
discards irrelevant data,
and then calculates the
statistics.
It generates the \code{\textbackslash{}OptimizationOverview} macro,
which creates \cref{fig:perf-impact-optimization}.

\begin{figure}[htb]
  \centering
	\OptimizationOverview{}
  \caption{Performance Impact of the Optimizations on the Average Peak Performance over all Benchmarks.
  The boxplot shows the performance of Moth normalized to the untyped version, \ie,
  without any type checks.
  The performance of Moth with both optimizations and Moth
  with only the node for optimized type checks are identical.
  The subtype check cache improves performance over the unoptimized version,
  but does not contribute to the peak performance.
  }
	\label{fig:perf-impact-optimization}
\end{figure}


\section{Transient Typechecks are (Almost) Free}

The final part of the evaluation looks into microbenchmarks
and impact on warmup performance.

The plots are generated by \code{type-cost.Rnw}, which
uses the data file produced by \code{rebench codespeed.conf type-cost}.
The file first loads the data and generates
macros \cref{fig:type-cost-micro},
\code{\textbackslash{}TypeCostFirstIt},
\code{\textbackslash{}TypeCostLastIt} that make the plots.

\begin{figure}
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \TypeCostFirstIt{}
    \caption{Iteration 1.}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.5\textwidth}
    \centering
    \TypeCostLastIt{}
    \caption{Iteration 100.}
  \end{subfigure}

	% \TypeCost{}
  \caption{Transient Typechecks are (Almost) Free.
    Two microbenchmarks demonstrate the common scenario that the addition
    of type annotations in our system does not have an impact on peak performance.
    The two microbenchmarks are measured in 6 variants, stepwise increasing the number
    of method arguments that have type annotations.
    Furthermore, we show the result for the first benchmark iteration and the
    one hundredth.
    Moth (neither), \ie, Moth without our two optimizations sees a linear increase in run time.
    For the first iteration, we see some difference between Moth (both) and Moth (untyped).
    By the hundredth iteration, however, the compiler has eliminated
    the overhead of the type checks
    and both Moth variants essentially have the same performance
    (independent of the number of method arguments with type annotations).}
	\label{fig:type-cost-micro}
\end{figure}

\end{document}
